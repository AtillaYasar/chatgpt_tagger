[
    {
        "title": "Mycroft TTS Plugin ElevenLabs:  https://github.com/KishCom/mycroft-tts-plugin-elevenlabs",
        "text": "@KishCom\nKishCom Elevenlabs beta TTS\n549dce5\non Jan 23\nGit stats\n 1 commit\nFiles\nType\nName\nLatest commit message\nCommit time\nmycroft_tts_plugin_elevenlabs\nElevenlabs beta TTS\n4 months ago\nREADME.md\nElevenlabs beta TTS\n4 months ago\nrequirements.txt\nElevenlabs beta TTS\n4 months ago\nsetup.py\nElevenlabs beta TTS\n4 months ago\nREADME.md\nmycroft-tts-plugin-elevenlabs\nThis TTS service for Mycroft requires a subscription to ElevenLabs. It is not very robust, but works well.\n\nConfiguration parameters:\n\n\"tts\": {\n    \"module\": \"elevenlabs_tts\",\n    \"elevenlabs\": {\n       \"voiceId\": \"THE_ID_OF_THE_VOICE_YOU_WANT\",\n       \"apiKey\": \"YOUR_ELEVENLABS_API_KEY\"\n    }\n}",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```Mycroft TTS Plugin ElevenLabs:  https://github.com/KishCom/mycroft-tts-plugin-elevenlabs\n@KishCom\nKishCom Elevenlabs beta TTS\n549dce5\non Jan 23\nGit stats\n 1 commit\nFiles\nType\nName\nLatest commit message\nCommit time\nmycroft_tts_plugin_elevenlabs\nElevenlabs beta TTS\n4 months ago\nREADME.md\nElevenlabs beta TTS\n4 months ago\nrequirements.txt\nElevenlabs beta TTS\n4 months ago\nsetup.py\nElevenlabs beta TTS\n4 months ago\nREADME.md\nmycroft-tts-plugin-elevenlabs\nThis TTS service for Mycroft requires a subscription to ElevenLabs. It is not very robust, but works well.\n\nConfiguration parameters:\n\n\"tts\": {\n    \"module\": \"elevenlabs_tts\",\n    \"elevenlabs\": {\n       \"voiceId\": \"THE_ID_OF_THE_VOICE_YOU_WANT\",\n       \"apiKey\": \"YOUR_ELEVENLABS_API_KEY\"\n    }\n}\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs' API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information.\n\n2) The text is a GitHub repository for a Mycroft TTS plugin that utilizes ElevenLabs' API. It provides a brief description of the plugin and its configuration parameters.\n\n3) \n- The GitHub repository is for a Mycroft TTS plugin that requires a subscription to ElevenLabs' API.\n- The plugin is not very robust, but works well.\n- The configuration parameters include the module name, \"elevenlabs_tts\", and the user's ElevenLabs API key and the ID of the voice they want to use."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "Based on the information provided in the text, it appears that the plugin utilizes the \"Text To Speech\" endpoint of ElevenLabs' API, as it requires the user to provide the ID of the voice they want to use and their API key. However, it is unclear if any other endpoints are used by the plugin."
            }
        ]
    },
    {
        "title": "Unofficial Python library: https://github.com/benbaptist/elevenlabs",
        "text": "About\nA simple unofficial Python3 library to interface with elevenlabs.io.\n\nResources\n Readme\nLicense\n MIT license\nStars\n 14 stars\nWatchers\n 3 watching\nForks\n 0 forks\nReport repository\nReleases\n 1 tags\nPackages\nNo packages published\nLanguages\nPython\n95.6%\n \nShell\n4.4%\n\n\n\nelevenlabs\nv0.1.1 - audiofile.save now returns path with filename and extension\n2 months ago\n.gitignore\npushed my pypi build script for the whole world to see\n2 months ago\nLICENSE\nsetup.py and MIT LICENSE\n3 months ago\nREADME.md\nfixed stickers\n2 weeks ago\nbuild.sh\npushed my pypi build script for the whole world to see\n2 months ago\nexample.py\nvastly improved example.py, cleaner API for fetching voices, etc.\n2 months ago\nsetup.cfg\nadded to pypi\n3 months ago\nsetup.py\nupdated with new info on official library, changed package name for l\u00e2\u20ac\u00a6\n2 weeks ago\nREADME.md\nelevenlabs\nPyPI - Downloads PyPI - Status PyPI\n\nNOTE: The official elevenlabs library is now available! Check it out here at this link. The PyPi package elevenlabs will now point to their official library moving forward. Just in case you need this library still, I went ahead and re-uploaded it to benbaptist-elevenlabs.\n\nelevenlabs is an unofficial Python library that provides an easy-to-use interface for elevenlabs.io' API. With this library, you can easily integrate with the API to generate voices.\n\nInstallation\nTo install this unofficial elevenlabs library, run:\n\npip install --upgrade benbaptist-elevenlabs\nThis library requires no special dependencies, and is written in pure Python.\n\nBasic Usage\nHere's an example of how you can use elevenlabs to retrieve data from the API:\n\nfrom elevenlabs import ElevenLabs\n\neleven = ElevenLabs(api_key)\n\n# Get a Voice object, by name or UUID\nvoice = eleven.voices[\"Arnold\"]\n\n# Generate the TTS\naudio = voice.generate(\"Hey buddy! It's a beautiful day.\")\n\n# Save the TTS to a file named 'my_first_tts' in the working directory\naudio.save(\"my_first_tts\")\nSee example.py for more. For more advanced usage, refer to the (currently non-existent!) documentation and the API reference.\n\nSee the official ElevenLabs documentation for more information.\n\nReal World Example\nI'm currently using this library to produce fully-automated weather reports on my radio station, BMIX94. For a sample of how it sounds, here's link to the latest weather report that will actually be broadcasting at various times throughout the day. That link is always pointing to the most recent weather report generated.",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```Unofficial Python library: https://github.com/benbaptist/elevenlabs\nAbout\nA simple unofficial Python3 library to interface with elevenlabs.io.\n\nResources\n Readme\nLicense\n MIT license\nStars\n 14 stars\nWatchers\n 3 watching\nForks\n 0 forks\nReport repository\nReleases\n 1 tags\nPackages\nNo packages published\nLanguages\nPython\n95.6%\n \nShell\n4.4%\n\n\n\nelevenlabs\nv0.1.1 - audiofile.save now returns path with filename and extension\n2 months ago\n.gitignore\npushed my pypi build script for the whole world to see\n2 months ago\nLICENSE\nsetup.py and MIT LICENSE\n3 months ago\nREADME.md\nfixed stickers\n2 weeks ago\nbuild.sh\npushed my pypi build script for the whole world to see\n2 months ago\nexample.py\nvastly improved example.py, cleaner API for fetching voices, etc.\n2 months ago\nsetup.cfg\nadded to pypi\n3 months ago\nsetup.py\nupdated with new info on official library, changed package name for l\u00e2\u20ac\u00a6\n2 weeks ago\nREADME.md\nelevenlabs\nPyPI - Downloads PyPI - Status PyPI\n\nNOTE: The official elevenlabs library is now available! Check it out here at this link. The PyPi package elevenlabs will now point to their official library moving forward. Just in case you need this library still, I went ahead and re-uploaded it to benbaptist-elevenlabs.\n\nelevenlabs is an unofficial Python library that provides an easy-to-use interface for elevenlabs.io' API. With this library, you can easily integrate with the API to generate voices.\n\nInstallation\nTo install this unofficial elevenlabs library, run:\n\npip install --upgrade benbaptist-elevenlabs\nThis library requires no special dependencies, and is written in pure Python.\n\nBasic Usage\nHere's an example of how you can use elevenlabs to retrieve data from the API:\n\nfrom elevenlabs import ElevenLabs\n\neleven = ElevenLabs(api_key)\n\n# Get a Voice object, by name or UUID\nvoice = eleven.voices[\"Arnold\"]\n\n# Generate the TTS\naudio = voice.generate(\"Hey buddy! It's a beautiful day.\")\n\n# Save the TTS to a file named 'my_first_tts' in the working directory\naudio.save(\"my_first_tts\")\nSee example.py for more. For more advanced usage, refer to the (currently non-existent!) documentation and the API reference.\n\nSee the official ElevenLabs documentation for more information.\n\nReal World Example\nI'm currently using this library to produce fully-automated weather reports on my radio station, BMIX94. For a sample of how it sounds, here's link to the latest weather report that will actually be broadcasting at various times throughout the day. That link is always pointing to the most recent weather report generated.\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs' API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information. \n\n2) The text is a description of an unofficial Python library that provides an interface for ElevenLabs' API. It includes information on the library's resources, installation, basic usage, and a real-world example of its use. \n\n3) \n- The text describes an unofficial Python library that interfaces with ElevenLabs' API.\n- The library provides an easy-to-use interface for generating voices using the API.\n- The library can be installed using pip.\n- The library requires no special dependencies and is written in pure Python.\n- The library includes an example of how to retrieve data from the API and generate TTS.\n- The library's documentation and API reference are currently non-existent.\n- The official ElevenLabs library is now available and the PyPi package for this unofficial library will now point to the official library moving forward. \n- The author of the text is using the library to produce fully-automated weather reports on their radio station."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text does not explicitly mention which endpoints are used by the unofficial Python library. However, based on the library's functionality, it is likely that it uses the `/v1/text-to-speech/{voice_id}` and `/v1/voices` endpoints to generate voices and access voice information, respectively."
            }
        ]
    },
    {
        "title": "ElevenLabs simple MP3 Generation: https://github.com/ThomasSilloway/ElevenLabsTTS",
        "text": "About\nRun ElevenLabs TTS to generate an mp3 & play it back\n\nResources\n Readme\nLicense\n MIT license\nStars\n 2 stars\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases 1\nv0.0.1\nLatest\non Feb 3\nPackages\nNo packages published\nLanguages\nPython\n99.3%\n \nBatchfile\n0.7%\n\n\n@ThomasSilloway\nThomasSilloway Update readme with usage\n25bba0f\n3 weeks ago\nGit stats\n 3 commits\nFiles\nType\nName\nLatest commit message\nCommit time\nconfig\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nsrc\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\n.gitignore\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nLICENSE\nInitial commit\n3 months ago\nREADME.md\nUpdate readme with usage\n3 weeks ago\nbuild.bat\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nrequirements.txt\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nsetup.py\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nREADME.md\nElevenLabsTTS\nRun ElevenLabs TTS to generate an mp3 & play it back\n\nExample usage:\n\npython -m src.main --text=\"Greetings This is Elevenlabs voice hi hi hih i love thomas\"",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```ElevenLabs simple MP3 Generation: https://github.com/ThomasSilloway/ElevenLabsTTS\nAbout\nRun ElevenLabs TTS to generate an mp3 & play it back\n\nResources\n Readme\nLicense\n MIT license\nStars\n 2 stars\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases 1\nv0.0.1\nLatest\non Feb 3\nPackages\nNo packages published\nLanguages\nPython\n99.3%\n \nBatchfile\n0.7%\n\n\n@ThomasSilloway\nThomasSilloway Update readme with usage\n25bba0f\n3 weeks ago\nGit stats\n 3 commits\nFiles\nType\nName\nLatest commit message\nCommit time\nconfig\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nsrc\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\n.gitignore\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nLICENSE\nInitial commit\n3 months ago\nREADME.md\nUpdate readme with usage\n3 weeks ago\nbuild.bat\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nrequirements.txt\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nsetup.py\nInitial build with support for passing in --text param to create TTS \u00e2\u20ac\u00a6\n3 months ago\nREADME.md\nElevenLabsTTS\nRun ElevenLabs TTS to generate an mp3 & play it back\n\nExample usage:\n\npython -m src.main --text=\"Greetings This is Elevenlabs voice hi hi hih i love thomas\"\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs' API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information.\n\n2) The text is a brief description of a GitHub repository called \"ElevenLabsTTS\" that allows users to generate an MP3 file using ElevenLabs' text-to-speech API and play it back. The text also includes an example usage command.\n\n3) \n- The text describes a GitHub repository called \"ElevenLabsTTS\"\n- The repository allows users to generate an MP3 file using ElevenLabs' text-to-speech API and play it back\n- The text includes an example usage command using the \"src.main\" module and passing in a text parameter\n- The repository is licensed under the MIT license\n- The repository has 2 stars and 1 watcher on GitHub\n- The repository has 3 commits and is primarily written in Python (99.3%) with a small amount of Batchfile code (0.7%)\n- The repository includes files for configuration, source code, ignoring files, licensing, building, requirements, and setup."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text does not explicitly mention which endpoints are used. However, it does mention that the repository allows users to generate an MP3 file using ElevenLabs' text-to-speech API, which suggests that the \"POST /v1/text-to-speech/{voice_id}\" or \"POST /v1/text-to-speech/{voice_id}/stream\" endpoints may be used. The example usage command also includes the text parameter, which suggests that the \"POST /v1/text-to-speech/{voice_id}\" endpoint may be used with the text parameter passed in."
            }
        ]
    },
    {
        "title": "Speech To Speech using ElevenLabs: https://github.com/lugia19/speechToSpeechElevenLabs",
        "text": "About\nSpeech to text to speech python script\n\nTopics\npython voice tts speech-recognition speech-to-speech elevenlabs\nResources\n Readme\nStars\n 13 stars\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n99.7%\n \nBatchfile\n0.3%\n\n\nmisc\nImprove OBS integration:\n2 months ago\nspeechRecognition\nFix some vosk weirdness.\n2 months ago\nttsProviders\nAdd latency reporting to console\n2 months ago\nREADME.md\nUpdate README.md\n2 months ago\nhelper.py\nMake the remember option optional.\n2 months ago\nrequirements.txt\nTextbox inputs marked as \"hidden\" now have their value saved to the s\u00e2\u20ac\u00a6\n2 months ago\nrun.bat\nUpdate run.bat\n2 months ago\nspeechToSpeech.py\nAdd latency reporting to console\n2 months ago\nREADME.md\nSpeech to text to speech using various services\nInfo\nThe main goal of the project is to offer speech to text to speech.\n\nIt now has a GUI, and it stores all the settings you input. Sensitive details such as API Keys are stored in the system keyring.\n\nIn case you want to use the cli, simply call the script from the comamnd line with the argument --cli.\n\nIt offers three separate speech recognition services:\n\nVosk, with recasepunc to add punctuation\nAzure speech recognition\nWhisper, both running locally and through openAI's API\nIn addition, it automatically translates the output into English, if the user is speaking a different language. Each speech recognition provider has different language support, so be sure to read the details.\n\nTranslation is provided via either DeepL for supported languages, or Google Translate.\n\nThe recognized and translated text is then sent to a TTS provider, of which two are supported:\n\nElevenlabs, through the elevenlabslib module, a high quality but paid online TTS service\npyttsx3, a low quality TTS that runs locally.\nThe project also allows you to synchronize the detected text with an OBS text source using obsws-python.\n\nInstallation and usage\nWarning: Python 3.11 is still not fully supported by pytorch (but it should work on the nightly build). I'd recommend using python 3.10.6\n\nBefore anything else: you'll need to have ffmpeg in your $PATH. You can follow this tutorial if you're on windows\n\nAdditionally, if you're on linux, you'll need to make sure portaudio is installed.\n\nOn windows:\n\nClone the repo: git clone https://github.com/lugia19/speechToSpeechElevenLabs.git\n\nRun run.bat - it will handle all the following steps for you.\n\nEverywhere else:\n\nClone the repo: git clone https://github.com/lugia19/speechToSpeechElevenLabs.git\n\nCreate a venv: python -m venv venv\n\nActivate the venv: venv\\Scripts\\activate\n\nIf you did it correctly, there should be (venv) at the start of the command line.\n\nInstall the requirements: pip install -r requirements.txt\n\nRun it.\n\nIf you would like to use the voice on something like discord, use VB-Cable. In the script select your normal microphone as input, VB-Cable input as the output, then on discord select VB-Cable output as the input. Yes, it's a little confusing.\n\nNotes on vosk/recasepunc\nIf you're looking to use the vosk/recasepunc and you need something besides the included (downloadable) models, read on.\n\nVosk models can be found here. The same page also offers some recasepunc models. For additional ones, you can look in the recasepunc repo.\n\nFor english I use vosk-model-en-us-0.22 and vosk-recasepunc-en-0.22. Recasepunc is technically optional when using vosk, but highly recommended to improve the output.\n\nThe script looks for models under the models/vosk and models/recasepunc folders.\n\nA typical folder structure would look something like this (recasepunc models can either be in their own folder or by themselves, depending on which source you download them from. Both are supported.):\n\n-misc\n-models\n    -vosk\n        -vosk-model-en-us-0.22\n        -vosk-model-it-0.22\n    -recasepunc\n        -vosk-recasepunc-en-0.22\n        it.22000\n-speechRecognition\n-ttsProviders\nhelper.py\nspeechToSpeech.py\nFor everything else, simply run the script and follow the instructions.\n\nIf you would like to use the voice on something like discord, use VB-Cable. In the script select your normal microphone as input, VB-Cable input as the output, then on discord select VB-Cable output as the input. Yes, it's a little confusing.",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```Speech To Speech using ElevenLabs: https://github.com/lugia19/speechToSpeechElevenLabs\nAbout\nSpeech to text to speech python script\n\nTopics\npython voice tts speech-recognition speech-to-speech elevenlabs\nResources\n Readme\nStars\n 13 stars\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n99.7%\n \nBatchfile\n0.3%\n\n\nmisc\nImprove OBS integration:\n2 months ago\nspeechRecognition\nFix some vosk weirdness.\n2 months ago\nttsProviders\nAdd latency reporting to console\n2 months ago\nREADME.md\nUpdate README.md\n2 months ago\nhelper.py\nMake the remember option optional.\n2 months ago\nrequirements.txt\nTextbox inputs marked as \"hidden\" now have their value saved to the s\u00e2\u20ac\u00a6\n2 months ago\nrun.bat\nUpdate run.bat\n2 months ago\nspeechToSpeech.py\nAdd latency reporting to console\n2 months ago\nREADME.md\nSpeech to text to speech using various services\nInfo\nThe main goal of the project is to offer speech to text to speech.\n\nIt now has a GUI, and it stores all the settings you input. Sensitive details such as API Keys are stored in the system keyring.\n\nIn case you want to use the cli, simply call the script from the comamnd line with the argument --cli.\n\nIt offers three separate speech recognition services:\n\nVosk, with recasepunc to add punctuation\nAzure speech recognition\nWhisper, both running locally and through openAI's API\nIn addition, it automatically translates the output into English, if the user is speaking a different language. Each speech recognition provider has different language support, so be sure to read the details.\n\nTranslation is provided via either DeepL for supported languages, or Google Translate.\n\nThe recognized and translated text is then sent to a TTS provider, of which two are supported:\n\nElevenlabs, through the elevenlabslib module, a high quality but paid online TTS service\npyttsx3, a low quality TTS that runs locally.\nThe project also allows you to synchronize the detected text with an OBS text source using obsws-python.\n\nInstallation and usage\nWarning: Python 3.11 is still not fully supported by pytorch (but it should work on the nightly build). I'd recommend using python 3.10.6\n\nBefore anything else: you'll need to have ffmpeg in your $PATH. You can follow this tutorial if you're on windows\n\nAdditionally, if you're on linux, you'll need to make sure portaudio is installed.\n\nOn windows:\n\nClone the repo: git clone https://github.com/lugia19/speechToSpeechElevenLabs.git\n\nRun run.bat - it will handle all the following steps for you.\n\nEverywhere else:\n\nClone the repo: git clone https://github.com/lugia19/speechToSpeechElevenLabs.git\n\nCreate a venv: python -m venv venv\n\nActivate the venv: venv\\Scripts\\activate\n\nIf you did it correctly, there should be (venv) at the start of the command line.\n\nInstall the requirements: pip install -r requirements.txt\n\nRun it.\n\nIf you would like to use the voice on something like discord, use VB-Cable. In the script select your normal microphone as input, VB-Cable input as the output, then on discord select VB-Cable output as the input. Yes, it's a little confusing.\n\nNotes on vosk/recasepunc\nIf you're looking to use the vosk/recasepunc and you need something besides the included (downloadable) models, read on.\n\nVosk models can be found here. The same page also offers some recasepunc models. For additional ones, you can look in the recasepunc repo.\n\nFor english I use vosk-model-en-us-0.22 and vosk-recasepunc-en-0.22. Recasepunc is technically optional when using vosk, but highly recommended to improve the output.\n\nThe script looks for models under the models/vosk and models/recasepunc folders.\n\nA typical folder structure would look something like this (recasepunc models can either be in their own folder or by themselves, depending on which source you download them from. Both are supported.):\n\n-misc\n-models\n    -vosk\n        -vosk-model-en-us-0.22\n        -vosk-model-it-0.22\n    -recasepunc\n        -vosk-recasepunc-en-0.22\n        it.22000\n-speechRecognition\n-ttsProviders\nhelper.py\nspeechToSpeech.py\nFor everything else, simply run the script and follow the instructions.\n\nIf you would like to use the voice on something like discord, use VB-Cable. In the script select your normal microphone as input, VB-Cable input as the output, then on discord select VB-Cable output as the input. Yes, it's a little confusing.\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs' API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information. \n\n2) The text is a description of a Python script called \"Speech to Speech using ElevenLabs\" that offers speech-to-text-to-speech functionality using various services. The script has a GUI and stores user settings, including API keys, in the system keyring. It offers three speech recognition services (Vosk, Azure, and Whisper) and two text-to-speech providers (Elevenlabs and pyttsx3). The recognized and translated text is sent to the chosen text-to-speech provider and can be synchronized with an OBS text source using obsws-python. The text also provides installation and usage instructions, including notes on using the script with Discord and customizing the Vosk models.\n\n3) \n- The text describes a Python script that offers speech-to-text-to-speech functionality using various services.\n- The script has a GUI and stores user settings, including API keys, in the system keyring.\n- The script offers three speech recognition services (Vosk, Azure, and Whisper) and two text-to-speech providers (Elevenlabs and pyttsx3).\n- The recognized and translated text is sent to the chosen text-to-speech provider and can be synchronized with an OBS text source using obsws-python.\n- The text provides installation and usage instructions, including notes on using the script with Discord and customizing the Vosk models."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text does not explicitly mention which endpoints are used from ElevenLabs' API. However, it does mention that the script uses Elevenlabs as one of its text-to-speech providers, which suggests that it may be using the `/v1/text-to-speech/{voice_id}` and `/v1/text-to-speech/{voice_id}/stream` endpoints. It is also possible that the script may be using other endpoints for accessing models and voices, managing samples and history, and user information, but this cannot be confirmed without further information."
            }
        ]
    },
    {
        "title": "Full python implementation of the elevenlabs API: https://github.com/lugia19/elevenlabslib",
        "text": "About\nFull python wrapper for the elevenlabs API.\n\nTopics\npython text-to-speech tts api-wrapper elevenlabs\nResources\n Readme\nLicense\n MIT license\nStars\n 99 stars\nWatchers\n 5 watching\nForks\n 15 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nUsed by 4\n@hu-po\n@Captain-Samy\n@oobabooga\n@lugia19\nContributors 3\n@lugia19\nlugia19\n@jmsether\njmsether\n@kevindublin\nkevindublin Vin Dublin\n\n\n\n@lugia19\nlugia19 Version bump.\ncc33896\nyesterday\nGit stats\n 96 commits\nFiles\nType\nName\nLatest commit message\nCommit time\ndocs\nVersion bump.\nyesterday\nelevenlabslib\nAdd the professional voice to the available classes.\nyesterday\n.readthedocs.yaml\npain. Just pain.\nlast week\nLICENSE\nFix things.\n3 months ago\nREADME.md\nAdd documentation link\nlast week\nexample.py\nMore updates and changes related to the docs.\nlast week\npyproject.toml\nVersion bump.\nyesterday\nREADME.md\nelevenlabslib\nPyPI - Downloads PyPI GitHub last commit\n\nPython wrapper for the full elevenlabs API.\n\nDocumentation now available at https://elevenlabslib.readthedocs.io/en/latest/\nInstallation\nJust run pip install elevenlabslib, it's on pypi.\n\nNote: On Linux, you may need to install portaudio. On debian and derivatives, it's sudo apt-get install libportaudio2, and possibly also sudo apt-get install python3-pyaudio.\n\nIMPORTANT: The library requires libsndfile v1.1.0 or newer, as that is when mp3 support was introduced. This won't be an issue on Windows, but may be relevant on other platforms. Check the soundfile repo for more information.\n\nUsage\nFor a far more comprehensive example, check example.py or the docs.\n\nHere is a very simple usage sample.\n\nRetrieves a voice based on the name\nPlays back (using the included playback functions that use sounddevice) all its samples (and the preview)\nGenerates and plays back a new audio\nDeletes the newly created audio from the user history\nfrom elevenlabslib import *\n\nuser = ElevenLabsUser(\"API_KEY\")\nvoice = user.get_voices_by_name(\"Rachel\")[0]  # This is a list because multiple voices can have the same name\n\nvoice.play_preview(playInBackground=False)\n\nvoice.generate_and_play_audio(\"Test.\", playInBackground=False)\n\nfor historyItem in user.get_history_items():\n    if historyItem.text == \"Test.\":\n        # The first items are the newest, so we can stop as soon as we find one.\n        historyItem.delete()\n        break",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```Full python implementation of the elevenlabs API: https://github.com/lugia19/elevenlabslib\nAbout\nFull python wrapper for the elevenlabs API.\n\nTopics\npython text-to-speech tts api-wrapper elevenlabs\nResources\n Readme\nLicense\n MIT license\nStars\n 99 stars\nWatchers\n 5 watching\nForks\n 15 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nUsed by 4\n@hu-po\n@Captain-Samy\n@oobabooga\n@lugia19\nContributors 3\n@lugia19\nlugia19\n@jmsether\njmsether\n@kevindublin\nkevindublin Vin Dublin\n\n\n\n@lugia19\nlugia19 Version bump.\ncc33896\nyesterday\nGit stats\n 96 commits\nFiles\nType\nName\nLatest commit message\nCommit time\ndocs\nVersion bump.\nyesterday\nelevenlabslib\nAdd the professional voice to the available classes.\nyesterday\n.readthedocs.yaml\npain. Just pain.\nlast week\nLICENSE\nFix things.\n3 months ago\nREADME.md\nAdd documentation link\nlast week\nexample.py\nMore updates and changes related to the docs.\nlast week\npyproject.toml\nVersion bump.\nyesterday\nREADME.md\nelevenlabslib\nPyPI - Downloads PyPI GitHub last commit\n\nPython wrapper for the full elevenlabs API.\n\nDocumentation now available at https://elevenlabslib.readthedocs.io/en/latest/\nInstallation\nJust run pip install elevenlabslib, it's on pypi.\n\nNote: On Linux, you may need to install portaudio. On debian and derivatives, it's sudo apt-get install libportaudio2, and possibly also sudo apt-get install python3-pyaudio.\n\nIMPORTANT: The library requires libsndfile v1.1.0 or newer, as that is when mp3 support was introduced. This won't be an issue on Windows, but may be relevant on other platforms. Check the soundfile repo for more information.\n\nUsage\nFor a far more comprehensive example, check example.py or the docs.\n\nHere is a very simple usage sample.\n\nRetrieves a voice based on the name\nPlays back (using the included playback functions that use sounddevice) all its samples (and the preview)\nGenerates and plays back a new audio\nDeletes the newly created audio from the user history\nfrom elevenlabslib import *\n\nuser = ElevenLabsUser(\"API_KEY\")\nvoice = user.get_voices_by_name(\"Rachel\")[0]  # This is a list because multiple voices can have the same name\n\nvoice.play_preview(playInBackground=False)\n\nvoice.generate_and_play_audio(\"Test.\", playInBackground=False)\n\nfor historyItem in user.get_history_items():\n    if historyItem.text == \"Test.\":\n        # The first items are the newest, so we can stop as soon as we find one.\n        historyItem.delete()\n        break\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for the ElevenLabs API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information.\n\n2) The text is a Python implementation of the ElevenLabs API, specifically a wrapper for the full API. It includes installation instructions, usage examples, and a link to documentation.\n\n3) \n- The text provides a Python implementation of the ElevenLabs API.\n- The implementation is a wrapper for the full API.\n- The wrapper includes installation instructions for the library.\n- The text provides a simple usage example for the wrapper.\n- The example retrieves a voice based on its name, plays back its samples and preview, generates and plays back new audio, and deletes the newly created audio from the user history.\n- The text includes a link to documentation for the wrapper."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text-to-speech endpoint is used in the example provided in the text. Specifically, the example retrieves a voice and generates and plays back new audio using the voice."
            }
        ]
    },
    {
        "title": "GPT-3 chatbot with ElevenLabs API: https://github.com/1nnovat1on/gpt_chatbot",
        "text": "About\nThis chatbot lets you use your microphone to communicate with GPT-4. It uses the Windows TTS to respond with a voice. It uses Pinecone to store long term information and retrieves it to create context. API keys for OpenAI and Pinecone required. Tested on Windows\n\nTopics\napi bot text-to-speech chatbot voice microphone artificial-intelligence tts voice-recognition speech-recognition chatbot-application voice-assistant pinecone gpt-3 gpt-4 llm langchain elevenlabs gpt-4-api langchain-python\nResources\n Readme\nStars\n 31 stars\nWatchers\n 4 watching\nForks\n 4 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n100.0%\n\n\n\n@1nnovat1on\n1nnovat1on Merge branch 'master' of https://github.com/1nnovat1on/gpt_chatbot\n51f9dc6\n2 days ago\nGit stats\n 33 commits\nFiles\nType\nName\nLatest commit message\nCommit time\n__pycache__\nfirst commit\n3 months ago\nREADME.md\nUpdate README.md\n2 days ago\nchatbot.py\nadded more Pinecone customization\n2 days ago\nlast message from prometheus.mp3\nupdated for GPT-4 and Pinecone\nlast month\nmemory.txt\nupdated for GPT-4 and Pinecone\nlast month\nrequirements.txt\nUpdate requirements.txt\n2 weeks ago\nvoiceTest.py\nfixed pinecone index reference\nlast month\nREADME.md\ngpt_chatbot\nThis chatbot allows you to use your microphone to communicate with GPT-4. --Formerly GPT-3\n\nIt features the Pinecone API to create a longterm memory... it will loop in bits of conversation you've had, best it can.\n\nAccounts and API keys for OpenAI and Pinecone required. You will also need to create an index in Pinecone.\n\nIt uses Windows TTS to respond, which might require some intial setup in your control panel/settings.\n\nTo start:\n\nInstall Python\nInstall git\nCopy this directory to your computer (preferably with git clone https://github.com/1nnovat1on/gpt_chatbot.git)\nOpen your terminal (CMD on Windows)\nSet environment variables called OPENAI_API_KEY and PINECONEKEY (use the setx OPENAI_API_KEY urkeygoeshere command)\nNavigate to the directory you downloaded the code to with cd. For me that means cd C:\\Documents\\gpt_chatbot\nRun pip install -r requirements.txt in your terminal\nCreate an index for Pinecone with [metric: cosine; Pod Type:p1x1; Dimensions: 1536]\nAdd your environment and index name for Pinecone in the #Pinecone section in chatbot.py\nRun python chatbot.py\nClick \"Click Me\"\n\nTalk away...\n\nHow to create a Pinecone index:\n\nAfter adding your API key to your environment variables, you'll need to create an index in the online Pinecone portal.\n\nBy default, your index should be named 'imalive' but you can name it something else if you change the variable in the code.\n\nThe index should have these attributes: [metric: cosine; Pod Type:p1x1; Dimensions: 1536]\n\nAlso, make note of what environment you use and change the code in the Pinecone section with the correct environment (e.g us-east1-gcp)\n\n4-4-23\nIt also uses ElevenLabs API to respond with a realistic voice. --Disabled for speed",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```GPT-3 chatbot with ElevenLabs API: https://github.com/1nnovat1on/gpt_chatbot\nAbout\nThis chatbot lets you use your microphone to communicate with GPT-4. It uses the Windows TTS to respond with a voice. It uses Pinecone to store long term information and retrieves it to create context. API keys for OpenAI and Pinecone required. Tested on Windows\n\nTopics\napi bot text-to-speech chatbot voice microphone artificial-intelligence tts voice-recognition speech-recognition chatbot-application voice-assistant pinecone gpt-3 gpt-4 llm langchain elevenlabs gpt-4-api langchain-python\nResources\n Readme\nStars\n 31 stars\nWatchers\n 4 watching\nForks\n 4 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n100.0%\n\n\n\n@1nnovat1on\n1nnovat1on Merge branch 'master' of https://github.com/1nnovat1on/gpt_chatbot\n51f9dc6\n2 days ago\nGit stats\n 33 commits\nFiles\nType\nName\nLatest commit message\nCommit time\n__pycache__\nfirst commit\n3 months ago\nREADME.md\nUpdate README.md\n2 days ago\nchatbot.py\nadded more Pinecone customization\n2 days ago\nlast message from prometheus.mp3\nupdated for GPT-4 and Pinecone\nlast month\nmemory.txt\nupdated for GPT-4 and Pinecone\nlast month\nrequirements.txt\nUpdate requirements.txt\n2 weeks ago\nvoiceTest.py\nfixed pinecone index reference\nlast month\nREADME.md\ngpt_chatbot\nThis chatbot allows you to use your microphone to communicate with GPT-4. --Formerly GPT-3\n\nIt features the Pinecone API to create a longterm memory... it will loop in bits of conversation you've had, best it can.\n\nAccounts and API keys for OpenAI and Pinecone required. You will also need to create an index in Pinecone.\n\nIt uses Windows TTS to respond, which might require some intial setup in your control panel/settings.\n\nTo start:\n\nInstall Python\nInstall git\nCopy this directory to your computer (preferably with git clone https://github.com/1nnovat1on/gpt_chatbot.git)\nOpen your terminal (CMD on Windows)\nSet environment variables called OPENAI_API_KEY and PINECONEKEY (use the setx OPENAI_API_KEY urkeygoeshere command)\nNavigate to the directory you downloaded the code to with cd. For me that means cd C:\\Documents\\gpt_chatbot\nRun pip install -r requirements.txt in your terminal\nCreate an index for Pinecone with [metric: cosine; Pod Type:p1x1; Dimensions: 1536]\nAdd your environment and index name for Pinecone in the #Pinecone section in chatbot.py\nRun python chatbot.py\nClick \"Click Me\"\n\nTalk away...\n\nHow to create a Pinecone index:\n\nAfter adding your API key to your environment variables, you'll need to create an index in the online Pinecone portal.\n\nBy default, your index should be named 'imalive' but you can name it something else if you change the variable in the code.\n\nThe index should have these attributes: [metric: cosine; Pod Type:p1x1; Dimensions: 1536]\n\nAlso, make note of what environment you use and change the code in the Pinecone section with the correct environment (e.g us-east1-gcp)\n\n4-4-23\nIt also uses ElevenLabs API to respond with a realistic voice. --Disabled for speed\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs API, which includes endpoints for text-to-speech, models, voices, samples, history, and user information.\n\n2) The text is a description of a GPT-3 chatbot that uses ElevenLabs API for text-to-speech functionality and Pinecone API for long-term memory storage. The chatbot allows users to communicate with GPT-4 using their microphone and responds with a voice using Windows TTS. The text provides instructions for setting up and running the chatbot, including creating an index in Pinecone and setting environment variables for API keys.\n\n3) \n- The text describes a GPT-3 chatbot that uses ElevenLabs API for text-to-speech functionality and Pinecone API for long-term memory storage.\n- The chatbot allows users to communicate with GPT-4 using their microphone and responds with a voice using Windows TTS.\n- The text provides instructions for setting up and running the chatbot, including creating an index in Pinecone and setting environment variables for API keys.\n- The chatbot uses Windows TTS to respond, which may require initial setup in the control panel/settings.\n- Accounts and API keys for OpenAI and Pinecone are required to use the chatbot.\n- The Pinecone API is used to create a long-term memory that loops in bits of conversation the user has had.\n- The text provides instructions for creating a Pinecone index with specific attributes.\n- The chatbot also uses ElevenLabs API to respond with a realistic voice, but this functionality is disabled for speed."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text mentions that the GPT-3 chatbot uses ElevenLabs API for text-to-speech functionality, but it does not specify which endpoints are used."
            }
        ]
    },
    {
        "title": "GPT-3 Speech To Text Chatbot (with ElevenLabs integration) : https://github.com/Adri6336/gpt3-speech-to-text-chatbot",
        "text": "About\nAllows you to have an engaging and safely emotive spoken / CLI conversation with the AI ChatGPT / GPT-4 while giving you the option to let it remember things discussed.\n\nTopics\npython cli ai memory chatbot tts voice-recognition speech-recognition openai customizable speech-to-text user-friendly conversational conversational-bots personalized conversational-ai gpt-3 gpt-4 chatgpt elevenlabs\nResources\n Readme\nLicense\n GPL-3.0 license\nStars\n 120 stars\nWatchers\n 2 watching\nForks\n 18 forks\nReport repository\nReleases 10\nGPT-VCC v1.1.3\nLatest\non Mar 25\n+ 9 releases\nSponsor this project\n@Adri6336\nAdri6336 Adrian\npatreon\npatreon.com/Adri6336\nLearn more about GitHub Sponsors\nLanguages\nPython\n100.0%\n\n\n\n.github\nCreate FUNDING.yml\n2 months ago\nmedia\nAdded voice command to toggle ElevenLabs TTS on and off. Updated comm\u00e2\u20ac\u00a6\n2 months ago\nLICENSE\nInitial commit\n4 months ago\nREADME.md\nUpdate README.md\n2 months ago\nbot.jpeg\nChanged image to reflect current state\n2 months ago\nchatbot.py\nFixed same bug in token recycler and memory generator. They had their\u00e2\u20ac\u00a6\n2 months ago\ngeneral_functions.py\nFixed bug where Google's TTS would hear 1 and transcribe 'one', or he\u00e2\u20ac\u00a6\n2 months ago\ngpt_prompts.py\nCreated placeholder file for getting different prompts based on the m\u00e2\u20ac\u00a6\n2 months ago\ngptcli.py\nFixed bug where gptcli was not allowing you to set tokens based on your\n2 months ago\nkeys.txt\nFixed bug where ElevenLabs key would not load correctly from file\n3 months ago\nmain.py\nFixed bug where gptcli was not allowing you to set tokens based on your\n2 months ago\nrequirements.txt\nUpdated chatbot to use ChatGPT as the default model. This will improv\u00e2\u20ac\u00a6\n2 months ago\ntts_functions.py\nMade changes to allow use of argparse for obtaining keys and passing \u00e2\u20ac\u00a6\n2 months ago\nREADME.md\ngpt-voice-conversation-chatbot (GPT-VCC) (formerly GPT3-STTC)\n(Used to be called gpt3-speech-to-text-chatbot, but was changed due to plans to add ability to swap model if desired)\n\nThis is a bot that allows you to have an engaging and safely emotive spoken conversation with ChatGPT or GPT-4 using your microphone. If you'd prefer to type rather than speak, you can also converse with the bot via the terminal.\n\nThe tool uses a modified GPT chat preset, handles keeping track of the conversation, and uses ChatGPT's API by default. You can tell GPT something and it will remember what you said for the session and you can also have the bot develop a memory of you over time if you'd like. Despite limitations based on GPT's max token count, GPT-VCC should still be able to converse with you for as long as you'd like without losing awareness about what you've talked about. In order to use this tool, you will need a valid OpenAI API key.\n\nThe bot requires OpenAI's moderation and GPT APIs to be working properly without too much latency. You can find the status here: https://status.openai.com/\n\nThe releases should be stable, as far as previous testing goes, but will not have all of the newest features. If you would like to have all the features as listed here, clone the repository and run 'git pull' every now and then. This will get you the newest features and bug fixes as they come, but it could be unstable.\n\nimage\n\n(Note: wiseTech is the name my bot instance chose for itself)\n\nInstalling\nFirst off, you'll need an OpenAI API key. You can create an account to get an API key here: https://openai.com/api/ .\n\nWindows\nDownload Python at https://www.python.org/\n\nDownload this repo either via the releases, git cloning the repo, or pressing the code button towards the upper right and pressing \"Download ZIP\".\n\nExtract contents, then move into folder with the files.\n\nIf you have Windows Terminal installed, right click the empty part of the folder and select 'Open in Terminal'. Otherwise, use Win + R and enter powershell. Once you're in a terminal window and at the proper directory, use \"pip install -r requirements.txt\". If this is done successfully, you should be ready to go as soon as you get yourself an OpenAI API key.\n\nFollow the steps listed in Using GPT-VCC\n\nLinux (Debian / Ubuntu based)\nInstall pip3\n\n sudo apt install python3-pip\nDownload this repo either via the releases, git cloning the repo, or pressing the code button towards the upper right and pressing \"Download ZIP\".\n\nExtract files, move into directory, open requirements.txt, and delete pyaudio==0.2.13 from file. Make it look like it was never there, preserving the original formatting.\n\nDownload pyaudio with apt as follows:\n\n sudo apt install python3-pyaudio\nDownload espeak with apt as follows (needed for interface communication):\n\n sudo apt install espeak\nDownload other requirements with pip as follows:\n\n pip3 install -r requirements.txt\nFollow the steps listed in Using GPT-VCC\n\nUsing GPT-VCC\nTo use this chatbot, enter the following command once you've navigated to the bot's folder (replacing <key> with your api key):\n\npython main.py <key>\nFor convenience, you can also just enter the key into the keys.txt file. When you run the script, the bot will automatically read this file and load the key.\n\nA Pygame gui will pop up; its colors represent the state of the bot. The color red indicates that the bot is not listening. To make the bot listen to you, press space. The color will then turn to yellow when its loading, then green when it's listening. Speak freely when the color is green, your speech will be recorded, converted to text, then fed to GPT if it is in compliance with OpenAI's policies. When GPT is ready to reply, the screen will turn blue.\n\nIf you would like to use the terminal, run gptcli.py instead using the same syntax (having keyfile also works):\n\npython gptcli.py <key>\nIf you would like to use ElevenLabs TTS, you must enter your personal ElevenLabs api key following your OpenAI api key as follows or fill in the key in the key file:\n\n    python main.py <OpenAI key> <ElevenLabs TTS key>\nIf you don't want to use the fancy TTS, this bot will automatically use Google's TTS.\n\nContent Moderation\nThe moderation uses both OpenAI's moderation tool and NLTK. Combined, they hope to prevent the use of GPT that is outside of OpenAI's useage policy. This is not an infaliable method though, so please exercise caution with what you give GPT.\n\nPlease note that outages or latency problems with the moderation api will prevent you from using this chatbot. If you must talk with the bot while OpenAI is having issues, please edit the chatbot.py file to exclude the \"not self.flagged_by_openai(text)\" condition. I do not recommend this though.\n\nCloned / Alternate ElevenLabs Voices\nYou can now use alternate voices if you wish using an argument passed when starting the program. In the future a more fleshed out way to do this is planned. For now, you'll need to know the ID of the voice you want to use (you can find a list of the base voice ID's here). Once you know that ID, use it as follows with the example ID \"21m00Tcm4TlvDq8ikWAM\" for Rachel:\n\n    python main.py --voice_id 21m00Tcm4TlvDq8ikWAM\nTo use cloned voices, do the following:\n\nGo to the voice lab at https://beta.elevenlabs.io/voice-lab and create a custom voice.\n\nOnce you've got a voice cloned, go here https://api.elevenlabs.io/docs#/voices/Get_voices_v1_voices_get .\n\nPress \"Try It Out\", enter your API key into the box, then press \"Execute\".\n\nBelow the execute button, you'll see a box labeled \"Response body\". Scroll down in this box until you find the voice you named. Get the \"voice_id\" that's directly above it, and use it as in the above example.\n\nControls\nKeyboard\nSPACEBAR: This starts a recording. Whatever you say will be then transcribed and sent to GPT (if it passes filters).\n\nESCAPE: This exits without memorizing.\n\nQ: This quits and has bot remember details about you and your conversations (data is saved in the text file called memories.txt)\n\nP: This attempts to cancel a request to GPT. It will either prevent transcribing of message or will avoid sending it to GPT.\n\nVoice Commands\nSay 'please set tokens to #': When the bot recognizes this phrase, it will try to set the max_tokens of the reply to the value you specified.\n\nSay 'speak like a robot': This will set all responses from GPT to be spoken with a robotic TTS program that works offline. In CLI mode, enter '!robospeak()' to toggle this mode.\n\nSay 'stop speaking like a robot': This will revert bot's TTS to whatever you had before (either Google or ElevenLabs TTS). In CLI mode, enter '!robospeak()' to toggle this mode.\n\nSay 'please display conversation': This will output your entire conversation to the terminal window.\n\nSay 'please display memories': This will provide an output of all memories saved into long term storage.\n\nSay 'please restore memory': This will attempt to repair the working memory of the bot by consolidating a certain number of memories from the long term storage .\n\nSay 'please set preset to': This will set the preset (a text string given to AI at start of every conversation) for the bot. For example, the preset 'speak like a pirate' makes AI speak like a pirate. You can find example presets here: https://github.com/Adri6336/gpt-voice-conversation-chatbot/wiki/Example-Presets .\n\nSay 'please reset preset': This will delete the preset you made.\n\nSay 'please set name to': This will set the name of the bot to whatever you specify, so long as it is in accordance with OpenAI's usage policies. After setting name, the bot will refer to itself by the name you set.\n\nSay 'please toggle gpt4': This will toggle between ChatGPT and GPT-4 models. On start up, your switch will be preserved. In CLI mode, enter '!gpt4()' to toggle the model.\n\nSay 'please set creativity to': This will set the bot's default randomness to a value you specify between 1 and 15 (used to be 9). In CLI mode, use '!creativity(#)' where the # sign is a value between 0.01 and 1.5.\n\nSay 'please list commands': This will have the bot list out the available commands for you.\n\nSay 'please toggle ElevenLabs': This will toggle the bot's use of ElevenLabs TTS on and off. In CLI mode, use !11ai() to toggle it.\n\nFeatures\nHave a personalized conversation with ChatGPT or GPT-4\n\nHear GPT talk to you with Google's TTS tool (will pronounce accents accurately if it can), in ElevenLab's life-like TTS (if you have a valid api key), or as a robot (say \"speak like a robot\" to activate)\n\nSpeak with GPT outloud using Google's speech recognition tech\n\nBot will remember things about you if you close with the 'Q' key\n\nSee GPT's replies as text in the terminal window. Most UTF-32 characters (like Chinese and Arabic text) will also be printed\n\nAutomatically save conversations to a file on your disk to help you keep track of what you've talked about\n\nSave a custom preset to have an experience better suited for you and your needs\n\nCustomize the bot's name\n\nCustomize the bot's creativity\n\nChat with GPT via the terminal (Windows and Linux)\n\nExample Use Cases\nConverse with bot recreationally\n\nUse bot via terminal to practice a language by a setting preset to talk to you in that language like a teacher, and entering '!speak()' with google TTS to pronounce the bot's reponses in that language properly\n\nHave bot help you practice programming by asking you questions and giving feedback on your code\n\nAsk bot questions over various things that pop up\n\nAsk bot to help you with writing cover letters and descriptions\n\nExample Videos of gptcli.py\nElevenLabs Voice Demonstration\n\n main-slim.mp4 \n\n\n\nGoogle TTS Spanish Voice Demonstration\n\n spanish-slim.mp4 ",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```GPT-3 Speech To Text Chatbot (with ElevenLabs integration) : https://github.com/Adri6336/gpt3-speech-to-text-chatbot\nAbout\nAllows you to have an engaging and safely emotive spoken / CLI conversation with the AI ChatGPT / GPT-4 while giving you the option to let it remember things discussed.\n\nTopics\npython cli ai memory chatbot tts voice-recognition speech-recognition openai customizable speech-to-text user-friendly conversational conversational-bots personalized conversational-ai gpt-3 gpt-4 chatgpt elevenlabs\nResources\n Readme\nLicense\n GPL-3.0 license\nStars\n 120 stars\nWatchers\n 2 watching\nForks\n 18 forks\nReport repository\nReleases 10\nGPT-VCC v1.1.3\nLatest\non Mar 25\n+ 9 releases\nSponsor this project\n@Adri6336\nAdri6336 Adrian\npatreon\npatreon.com/Adri6336\nLearn more about GitHub Sponsors\nLanguages\nPython\n100.0%\n\n\n\n.github\nCreate FUNDING.yml\n2 months ago\nmedia\nAdded voice command to toggle ElevenLabs TTS on and off. Updated comm\u00e2\u20ac\u00a6\n2 months ago\nLICENSE\nInitial commit\n4 months ago\nREADME.md\nUpdate README.md\n2 months ago\nbot.jpeg\nChanged image to reflect current state\n2 months ago\nchatbot.py\nFixed same bug in token recycler and memory generator. They had their\u00e2\u20ac\u00a6\n2 months ago\ngeneral_functions.py\nFixed bug where Google's TTS would hear 1 and transcribe 'one', or he\u00e2\u20ac\u00a6\n2 months ago\ngpt_prompts.py\nCreated placeholder file for getting different prompts based on the m\u00e2\u20ac\u00a6\n2 months ago\ngptcli.py\nFixed bug where gptcli was not allowing you to set tokens based on your\n2 months ago\nkeys.txt\nFixed bug where ElevenLabs key would not load correctly from file\n3 months ago\nmain.py\nFixed bug where gptcli was not allowing you to set tokens based on your\n2 months ago\nrequirements.txt\nUpdated chatbot to use ChatGPT as the default model. This will improv\u00e2\u20ac\u00a6\n2 months ago\ntts_functions.py\nMade changes to allow use of argparse for obtaining keys and passing \u00e2\u20ac\u00a6\n2 months ago\nREADME.md\ngpt-voice-conversation-chatbot (GPT-VCC) (formerly GPT3-STTC)\n(Used to be called gpt3-speech-to-text-chatbot, but was changed due to plans to add ability to swap model if desired)\n\nThis is a bot that allows you to have an engaging and safely emotive spoken conversation with ChatGPT or GPT-4 using your microphone. If you'd prefer to type rather than speak, you can also converse with the bot via the terminal.\n\nThe tool uses a modified GPT chat preset, handles keeping track of the conversation, and uses ChatGPT's API by default. You can tell GPT something and it will remember what you said for the session and you can also have the bot develop a memory of you over time if you'd like. Despite limitations based on GPT's max token count, GPT-VCC should still be able to converse with you for as long as you'd like without losing awareness about what you've talked about. In order to use this tool, you will need a valid OpenAI API key.\n\nThe bot requires OpenAI's moderation and GPT APIs to be working properly without too much latency. You can find the status here: https://status.openai.com/\n\nThe releases should be stable, as far as previous testing goes, but will not have all of the newest features. If you would like to have all the features as listed here, clone the repository and run 'git pull' every now and then. This will get you the newest features and bug fixes as they come, but it could be unstable.\n\nimage\n\n(Note: wiseTech is the name my bot instance chose for itself)\n\nInstalling\nFirst off, you'll need an OpenAI API key. You can create an account to get an API key here: https://openai.com/api/ .\n\nWindows\nDownload Python at https://www.python.org/\n\nDownload this repo either via the releases, git cloning the repo, or pressing the code button towards the upper right and pressing \"Download ZIP\".\n\nExtract contents, then move into folder with the files.\n\nIf you have Windows Terminal installed, right click the empty part of the folder and select 'Open in Terminal'. Otherwise, use Win + R and enter powershell. Once you're in a terminal window and at the proper directory, use \"pip install -r requirements.txt\". If this is done successfully, you should be ready to go as soon as you get yourself an OpenAI API key.\n\nFollow the steps listed in Using GPT-VCC\n\nLinux (Debian / Ubuntu based)\nInstall pip3\n\n sudo apt install python3-pip\nDownload this repo either via the releases, git cloning the repo, or pressing the code button towards the upper right and pressing \"Download ZIP\".\n\nExtract files, move into directory, open requirements.txt, and delete pyaudio==0.2.13 from file. Make it look like it was never there, preserving the original formatting.\n\nDownload pyaudio with apt as follows:\n\n sudo apt install python3-pyaudio\nDownload espeak with apt as follows (needed for interface communication):\n\n sudo apt install espeak\nDownload other requirements with pip as follows:\n\n pip3 install -r requirements.txt\nFollow the steps listed in Using GPT-VCC\n\nUsing GPT-VCC\nTo use this chatbot, enter the following command once you've navigated to the bot's folder (replacing <key> with your api key):\n\npython main.py <key>\nFor convenience, you can also just enter the key into the keys.txt file. When you run the script, the bot will automatically read this file and load the key.\n\nA Pygame gui will pop up; its colors represent the state of the bot. The color red indicates that the bot is not listening. To make the bot listen to you, press space. The color will then turn to yellow when its loading, then green when it's listening. Speak freely when the color is green, your speech will be recorded, converted to text, then fed to GPT if it is in compliance with OpenAI's policies. When GPT is ready to reply, the screen will turn blue.\n\nIf you would like to use the terminal, run gptcli.py instead using the same syntax (having keyfile also works):\n\npython gptcli.py <key>\nIf you would like to use ElevenLabs TTS, you must enter your personal ElevenLabs api key following your OpenAI api key as follows or fill in the key in the key file:\n\n    python main.py <OpenAI key> <ElevenLabs TTS key>\nIf you don't want to use the fancy TTS, this bot will automatically use Google's TTS.\n\nContent Moderation\nThe moderation uses both OpenAI's moderation tool and NLTK. Combined, they hope to prevent the use of GPT that is outside of OpenAI's useage policy. This is not an infaliable method though, so please exercise caution with what you give GPT.\n\nPlease note that outages or latency problems with the moderation api will prevent you from using this chatbot. If you must talk with the bot while OpenAI is having issues, please edit the chatbot.py file to exclude the \"not self.flagged_by_openai(text)\" condition. I do not recommend this though.\n\nCloned / Alternate ElevenLabs Voices\nYou can now use alternate voices if you wish using an argument passed when starting the program. In the future a more fleshed out way to do this is planned. For now, you'll need to know the ID of the voice you want to use (you can find a list of the base voice ID's here). Once you know that ID, use it as follows with the example ID \"21m00Tcm4TlvDq8ikWAM\" for Rachel:\n\n    python main.py --voice_id 21m00Tcm4TlvDq8ikWAM\nTo use cloned voices, do the following:\n\nGo to the voice lab at https://beta.elevenlabs.io/voice-lab and create a custom voice.\n\nOnce you've got a voice cloned, go here https://api.elevenlabs.io/docs#/voices/Get_voices_v1_voices_get .\n\nPress \"Try It Out\", enter your API key into the box, then press \"Execute\".\n\nBelow the execute button, you'll see a box labeled \"Response body\". Scroll down in this box until you find the voice you named. Get the \"voice_id\" that's directly above it, and use it as in the above example.\n\nControls\nKeyboard\nSPACEBAR: This starts a recording. Whatever you say will be then transcribed and sent to GPT (if it passes filters).\n\nESCAPE: This exits without memorizing.\n\nQ: This quits and has bot remember details about you and your conversations (data is saved in the text file called memories.txt)\n\nP: This attempts to cancel a request to GPT. It will either prevent transcribing of message or will avoid sending it to GPT.\n\nVoice Commands\nSay 'please set tokens to #': When the bot recognizes this phrase, it will try to set the max_tokens of the reply to the value you specified.\n\nSay 'speak like a robot': This will set all responses from GPT to be spoken with a robotic TTS program that works offline. In CLI mode, enter '!robospeak()' to toggle this mode.\n\nSay 'stop speaking like a robot': This will revert bot's TTS to whatever you had before (either Google or ElevenLabs TTS). In CLI mode, enter '!robospeak()' to toggle this mode.\n\nSay 'please display conversation': This will output your entire conversation to the terminal window.\n\nSay 'please display memories': This will provide an output of all memories saved into long term storage.\n\nSay 'please restore memory': This will attempt to repair the working memory of the bot by consolidating a certain number of memories from the long term storage .\n\nSay 'please set preset to': This will set the preset (a text string given to AI at start of every conversation) for the bot. For example, the preset 'speak like a pirate' makes AI speak like a pirate. You can find example presets here: https://github.com/Adri6336/gpt-voice-conversation-chatbot/wiki/Example-Presets .\n\nSay 'please reset preset': This will delete the preset you made.\n\nSay 'please set name to': This will set the name of the bot to whatever you specify, so long as it is in accordance with OpenAI's usage policies. After setting name, the bot will refer to itself by the name you set.\n\nSay 'please toggle gpt4': This will toggle between ChatGPT and GPT-4 models. On start up, your switch will be preserved. In CLI mode, enter '!gpt4()' to toggle the model.\n\nSay 'please set creativity to': This will set the bot's default randomness to a value you specify between 1 and 15 (used to be 9). In CLI mode, use '!creativity(#)' where the # sign is a value between 0.01 and 1.5.\n\nSay 'please list commands': This will have the bot list out the available commands for you.\n\nSay 'please toggle ElevenLabs': This will toggle the bot's use of ElevenLabs TTS on and off. In CLI mode, use !11ai() to toggle it.\n\nFeatures\nHave a personalized conversation with ChatGPT or GPT-4\n\nHear GPT talk to you with Google's TTS tool (will pronounce accents accurately if it can), in ElevenLab's life-like TTS (if you have a valid api key), or as a robot (say \"speak like a robot\" to activate)\n\nSpeak with GPT outloud using Google's speech recognition tech\n\nBot will remember things about you if you close with the 'Q' key\n\nSee GPT's replies as text in the terminal window. Most UTF-32 characters (like Chinese and Arabic text) will also be printed\n\nAutomatically save conversations to a file on your disk to help you keep track of what you've talked about\n\nSave a custom preset to have an experience better suited for you and your needs\n\nCustomize the bot's name\n\nCustomize the bot's creativity\n\nChat with GPT via the terminal (Windows and Linux)\n\nExample Use Cases\nConverse with bot recreationally\n\nUse bot via terminal to practice a language by a setting preset to talk to you in that language like a teacher, and entering '!speak()' with google TTS to pronounce the bot's reponses in that language properly\n\nHave bot help you practice programming by asking you questions and giving feedback on your code\n\nAsk bot questions over various things that pop up\n\nAsk bot to help you with writing cover letters and descriptions\n\nExample Videos of gptcli.py\nElevenLabs Voice Demonstration\n\n main-slim.mp4 \n\n\n\nGoogle TTS Spanish Voice Demonstration\n\n spanish-slim.mp4 \n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs API, which includes endpoints for text-to-speech, models, voices, samples, history, and user information. \n2) The text is a description of a GPT-3 Speech to Text Chatbot that integrates with ElevenLabs API. It allows users to have a spoken or CLI conversation with the AI ChatGPT or GPT-4, with the option to remember things discussed. The bot uses a modified GPT chat preset and ChatGPT's API by default. The text provides instructions for installing and using the chatbot, as well as a breakdown of its features and example use cases. \n3) \n- The text describes a GPT-3 Speech to Text Chatbot that integrates with ElevenLabs API.\n- The chatbot allows users to have a spoken or CLI conversation with the AI ChatGPT or GPT-4, with the option to remember things discussed.\n- The bot uses a modified GPT chat preset and ChatGPT's API by default.\n- The text provides instructions for installing and using the chatbot, as well as a breakdown of its features and example use cases.\n- The chatbot requires a valid OpenAI API key and relies on OpenAI's moderation and GPT APIs to be working properly without too much latency.\n- The text provides controls for using the chatbot, including keyboard and voice commands.\n- The chatbot can use Google's TTS tool, ElevenLab's life-like TTS, or a robotic TTS program to talk to users.\n- The text provides example videos of the chatbot in action."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text does not explicitly mention which endpoints are used from ElevenLabs API. However, it does mention that the chatbot requires a valid OpenAI API key and relies on OpenAI's moderation and GPT APIs to be working properly without too much latency."
            }
        ]
    },
    {
        "title": "LiveSynth - generates AI voice using Whisper and ElevenLabs API: https://github.com/babbaj/LiveSynth",
        "text": "About\nNo description, website, or topics provided.\nResources\n Readme\nLicense\n GPL-3.0 license\nStars\n 3 stars\nWatchers\n 2 watching\nForks\n 1 fork\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n84.3%\n \nNix\n15.7%\n\n\n\n@babbaj\nbabbaj force cudnn initialization\n3b6ffc6\n2 weeks ago\nGit stats\n 22 commits\nFiles\nType\nName\nLatest commit message\nCommit time\n.envrc\ninitial commit\n3 months ago\nLICENSE\nCreate LICENSE\n3 months ago\nREADME.MD\nsmall readme mistake\n3 months ago\nflake.lock\ninitial commit\n3 months ago\nflake.nix\nAdd support for openai whisper api\n3 weeks ago\nmain.py\nforce cudnn initialization\n2 weeks ago\nREADME.MD\nLiveSynth\nLiveSynth is a program that allows you to generate a voice with AI using OpenAI's Whisper and the ElevenLabs API. By holding a key, you can record your voice, have it transcribed, and used to generate an AI voice from what you said. I use this to give myself an interesting voice in games. Windows is not and will not be supported lol.\n\nRequirements\nLinux\nPipeWire or PulseAudio\nCUDA\nUsage\npython whisper.py --key alt_r --voice 21m00Tcm4TlvDq8ikWAM --api-key <your xi-api-key>\n\nThe --key option expects the name of an x11 keysym, those are listed here.\n\nThe default whisper model requires 5GB of vram. This can be changed with --model or CPU transcription can be used with --cpu. The available whisper models are documented here.\n\nTo output to a custom sink (useful for using as an input device) you can use the --output-sink option which expects the name or serial of the sink.\n\n--input-source is also available to choose a specific input.\n\nCreating a virtual sink (named LiveSynth) and source (named LiveSynthSource) can be done with a couple commands:\n\npactl load-module module-null-sink sink_name=\"LiveSynth\" sink_properties=device.description=\"LiveSynth Sink\"\npactl load-module module-remap-source master=\"LiveSynth.monitor\" source_name=\"LiveSynthSource\" source_properties=device.description=\"LiveSynth Source\"\nCaveats\nWhisper uses a lot of VRAM (5GB for medium)\nThe ElevenLabs API often takes a while to return a result. Hopefully open source AI voice will catch up soon because this sucks.",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```LiveSynth - generates AI voice using Whisper and ElevenLabs API: https://github.com/babbaj/LiveSynth\nAbout\nNo description, website, or topics provided.\nResources\n Readme\nLicense\n GPL-3.0 license\nStars\n 3 stars\nWatchers\n 2 watching\nForks\n 1 fork\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n84.3%\n \nNix\n15.7%\n\n\n\n@babbaj\nbabbaj force cudnn initialization\n3b6ffc6\n2 weeks ago\nGit stats\n 22 commits\nFiles\nType\nName\nLatest commit message\nCommit time\n.envrc\ninitial commit\n3 months ago\nLICENSE\nCreate LICENSE\n3 months ago\nREADME.MD\nsmall readme mistake\n3 months ago\nflake.lock\ninitial commit\n3 months ago\nflake.nix\nAdd support for openai whisper api\n3 weeks ago\nmain.py\nforce cudnn initialization\n2 weeks ago\nREADME.MD\nLiveSynth\nLiveSynth is a program that allows you to generate a voice with AI using OpenAI's Whisper and the ElevenLabs API. By holding a key, you can record your voice, have it transcribed, and used to generate an AI voice from what you said. I use this to give myself an interesting voice in games. Windows is not and will not be supported lol.\n\nRequirements\nLinux\nPipeWire or PulseAudio\nCUDA\nUsage\npython whisper.py --key alt_r --voice 21m00Tcm4TlvDq8ikWAM --api-key <your xi-api-key>\n\nThe --key option expects the name of an x11 keysym, those are listed here.\n\nThe default whisper model requires 5GB of vram. This can be changed with --model or CPU transcription can be used with --cpu. The available whisper models are documented here.\n\nTo output to a custom sink (useful for using as an input device) you can use the --output-sink option which expects the name or serial of the sink.\n\n--input-source is also available to choose a specific input.\n\nCreating a virtual sink (named LiveSynth) and source (named LiveSynthSource) can be done with a couple commands:\n\npactl load-module module-null-sink sink_name=\"LiveSynth\" sink_properties=device.description=\"LiveSynth Sink\"\npactl load-module module-remap-source master=\"LiveSynth.monitor\" source_name=\"LiveSynthSource\" source_properties=device.description=\"LiveSynth Source\"\nCaveats\nWhisper uses a lot of VRAM (5GB for medium)\nThe ElevenLabs API often takes a while to return a result. Hopefully open source AI voice will catch up soon because this sucks.\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) General description of the background info:\nThe background info provides documentation for ElevenLabs API, which offers text-to-speech services, access to different models and voices, and samples. It also includes information on accessing user subscription and history.\n\n2) General description of the text:\nThe text is a README file for a program called LiveSynth, which generates an AI voice using OpenAI's Whisper and ElevenLabs API. The program allows users to record their voice, have it transcribed, and used to generate an AI voice. The README file provides information on the requirements for using the program, how to use it, and some caveats.\n\n3) Bulletpoint breakdown of the text:\n- LiveSynth is a program that generates an AI voice using OpenAI's Whisper and ElevenLabs API\n- Users can record their voice, have it transcribed, and used to generate an AI voice\n- The program requires Linux, PipeWire or PulseAudio, and CUDA\n- Users can specify the x11 keysym for recording their voice and the voice ID for generating the AI voice\n- The default whisper model requires 5GB of VRAM, but this can be changed with the --model or --cpu options\n- Users can output to a custom sink or choose a specific input with the --output-sink and --input-source options\n- The README file provides commands for creating a virtual sink and source\n- The ElevenLabs API can take a while to return a result, and Whisper uses a lot of VRAM."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The README file for LiveSynth does not explicitly mention which endpoints are used from the ElevenLabs API. However, it does mention that the program requires the user to provide an API key for ElevenLabs, and it specifies the voice ID for generating the AI voice. Therefore, it is likely that the program uses one or more of the following endpoints from the ElevenLabs API:\n\n- POST /v1/text-to-speech/{voice_id}\n- POST /v1/text-to-speech/{voice_id}/stream\n- GET /v1/models\n- GET /v1/voices\n- GET /v1/voices/settings/default\n- GET /v1/voices/{voice_id}/settings\n- GET /v1/voices/{voice_id}\n- DELETE /v1/voices/{voice_id}\n- POST /v1/voices/{voice_id}/settings/edit\n- POST /v1/voices/add\n- POST /v1/voices/{voice_id}/edit\n- DELETE /v1/voices/{voice_id}/samples/{sample_id}\n- GET /v1/voices/{voice_id}/samples/{sample_id}/audio\n- GET /v1/history\n- GET /v1/history/{history_item_id}/audio\n- DELETE /v1/history/{history_item_id}\n- POST /v1/history/download\n\nWithout further information, it is not possible to determine which specific endpoints are used by LiveSynth."
            }
        ]
    },
    {
        "title": "RenpyDialogToAudio: https://github.com/lugia19/renpyDialogToAudio",
        "text": "About\nTakes a renpy dialog export and generates voices using elevenlabs\n\nTopics\ntext-to-speech tts renpy renpy-engine elevenlabs\nResources\n Readme\nStars\n 3 stars\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n97.1%\n \nBatchfile\n2.9%\n\n\n\n@lugia19\nlugia19 Update run.bat\n\u00e2\u20ac\u00a6\n52f2943\non Mar 25\nGit stats\n 9 commits\nFiles\nType\nName\nLatest commit message\nCommit time\ndialogVoiceGenerator.py\nChange window name\n2 months ago\nreadme.md\nUpdate readme.md\n2 months ago\nrequirements.txt\nInitial commit\n2 months ago\nrun.bat\nUpdate run.bat\n2 months ago\nreadme.md\nRenpy dialog voice generator\nGenerates voice files based on a dialog.tab file (with text tags stripped).\n\nThe TTS is generated using elevenlabs, with my elevenlabslib library.\n\nWhen you run the program, it will show a GUI that will prompt you to specify:\n\nThe dialog file\nThe directory in which to save the audio files\nYour API key (will be stored in the system keyring for later runs)\nAfter this it will pull a list of all voices available to your elevenlabs account, and will ask you to assign a voice to each character found in the dialog export.\n\nIt will then generate the audio files and save them as {identifier}.mp3, ready to be used with the auto voice option.\n\nIn case you run out of credits (or the generation of files is interrupted for some other reason) the program will detect that there are existing audio files and will give you the option to skip the ones that already exist.\n\nSee here for an installation and usage video.\n\nInstallation\nDownload this repo git clone https://github.com/lugia19/renpyDialogToAudio.git\nOpen run.bat\nIn case you're not on windows, run.bat just creates a venv and install the requirements from requirements.txt, then runs the script, so you can just do that manually.\n\nIn addition, it will also perform a git pull whenever it's run.",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```RenpyDialogToAudio: https://github.com/lugia19/renpyDialogToAudio\nAbout\nTakes a renpy dialog export and generates voices using elevenlabs\n\nTopics\ntext-to-speech tts renpy renpy-engine elevenlabs\nResources\n Readme\nStars\n 3 stars\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n97.1%\n \nBatchfile\n2.9%\n\n\n\n@lugia19\nlugia19 Update run.bat\n\u00e2\u20ac\u00a6\n52f2943\non Mar 25\nGit stats\n 9 commits\nFiles\nType\nName\nLatest commit message\nCommit time\ndialogVoiceGenerator.py\nChange window name\n2 months ago\nreadme.md\nUpdate readme.md\n2 months ago\nrequirements.txt\nInitial commit\n2 months ago\nrun.bat\nUpdate run.bat\n2 months ago\nreadme.md\nRenpy dialog voice generator\nGenerates voice files based on a dialog.tab file (with text tags stripped).\n\nThe TTS is generated using elevenlabs, with my elevenlabslib library.\n\nWhen you run the program, it will show a GUI that will prompt you to specify:\n\nThe dialog file\nThe directory in which to save the audio files\nYour API key (will be stored in the system keyring for later runs)\nAfter this it will pull a list of all voices available to your elevenlabs account, and will ask you to assign a voice to each character found in the dialog export.\n\nIt will then generate the audio files and save them as {identifier}.mp3, ready to be used with the auto voice option.\n\nIn case you run out of credits (or the generation of files is interrupted for some other reason) the program will detect that there are existing audio files and will give you the option to skip the ones that already exist.\n\nSee here for an installation and usage video.\n\nInstallation\nDownload this repo git clone https://github.com/lugia19/renpyDialogToAudio.git\nOpen run.bat\nIn case you're not on windows, run.bat just creates a venv and install the requirements from requirements.txt, then runs the script, so you can just do that manually.\n\nIn addition, it will also perform a git pull whenever it's run.\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs' API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information. \n2) The text is a description of a GitHub repository called RenpyDialogToAudio, which is a tool that takes a Renpy dialog export and generates voices using ElevenLabs' API. It includes a GUI that prompts the user to specify the dialog file, directory to save the audio files, and API key. It also pulls a list of available voices and allows the user to assign a voice to each character in the dialog export. The audio files are saved as {identifier}.mp3 and can be used with the auto voice option. \n3) \n- RenpyDialogToAudio is a GitHub repository that generates voices using ElevenLabs' API.\n- It takes a Renpy dialog export and prompts the user to specify the dialog file, directory to save the audio files, and API key.\n- It pulls a list of available voices and allows the user to assign a voice to each character in the dialog export.\n- The audio files are saved as {identifier}.mp3 and can be used with the auto voice option.\n- The tool includes a GUI and can detect existing audio files to skip generating them again.\n- The installation involves downloading the repository, running the run.bat file (which creates a venv and installs requirements), and performing a git pull whenever it's run."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "Based on the information provided in the text, it appears that the RenpyDialogToAudio tool uses the `/v1/text-to-speech/{voice_id}` endpoint of ElevenLabs' API for generating the audio files. It also pulls a list of available voices using the `/v1/voices` endpoint and allows the user to assign a voice to each character in the dialog export. However, it is possible that other endpoints are used as well, but this information is not explicitly stated in the text."
            }
        ]
    },
    {
        "title": "https://github.com/lugia19/ElvenLabsWhisperXSplit",
        "text": "About\nExample of using whisperX and pydub to cut out a specific section of audio.\n\nResources\n Readme\nStars\n 1 star\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n100.0%\n\n\n@lugia19\nlugia19 Make it regenerate the audio (oops)\neb62d0c\n5 days ago\nGit stats\n 3 commits\nFiles\nType\nName\nLatest commit message\nCommit time\nREADME.md\nInitial commit\n5 days ago\nmain.py\nMake it regenerate the audio (oops)\n5 days ago\npinnedrequirements.txt\nInitial commit\n5 days ago\nREADME.md\nElevenlabs + WhisperX autosplicing via pydub\nmain.py contains a function that, given two strings (one being the actual \"spoken\" text and one being the text used to give it emotion), does the following:\n\nGenerates audio.mp3 containing the full text via elevenlabslib.\nUses whisperX to obtain word-level timestamps.\nIdentifies the start and end of the spoken text.\nUses pydub to create cut_audio.mp3, containing only the audio for the spoken text.\nTo try it out (I recommend doing this in a venv):\n\nDo pip install -r pinnedrequirements.txt --no-deps\nEdit api_key in main.py to contain your API key\nRun it\nThere is a dependency conflict going on due to pyannote-audio requiring an older version of soundfile than the minimum for mp3 support (which elevenlabslib requires) but it works fine if you just force it, hence the requirements file with pinned versions.",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```https://github.com/lugia19/ElvenLabsWhisperXSplit\nAbout\nExample of using whisperX and pydub to cut out a specific section of audio.\n\nResources\n Readme\nStars\n 1 star\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nPython\n100.0%\n\n\n@lugia19\nlugia19 Make it regenerate the audio (oops)\neb62d0c\n5 days ago\nGit stats\n 3 commits\nFiles\nType\nName\nLatest commit message\nCommit time\nREADME.md\nInitial commit\n5 days ago\nmain.py\nMake it regenerate the audio (oops)\n5 days ago\npinnedrequirements.txt\nInitial commit\n5 days ago\nREADME.md\nElevenlabs + WhisperX autosplicing via pydub\nmain.py contains a function that, given two strings (one being the actual \"spoken\" text and one being the text used to give it emotion), does the following:\n\nGenerates audio.mp3 containing the full text via elevenlabslib.\nUses whisperX to obtain word-level timestamps.\nIdentifies the start and end of the spoken text.\nUses pydub to create cut_audio.mp3, containing only the audio for the spoken text.\nTo try it out (I recommend doing this in a venv):\n\nDo pip install -r pinnedrequirements.txt --no-deps\nEdit api_key in main.py to contain your API key\nRun it\nThere is a dependency conflict going on due to pyannote-audio requiring an older version of soundfile than the minimum for mp3 support (which elevenlabslib requires) but it works fine if you just force it, hence the requirements file with pinned versions.\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for the API of ElevenLabs, which offers text-to-speech services. The API allows users to access different models, voices, and samples, as well as edit voice settings and generate audio files. \n\n2) The text is a description of a Python script that uses the ElevenLabs API, along with the whisperX and pydub libraries, to generate and splice audio files. The script takes two strings as input, one for the spoken text and one for the text used to give it emotion, and generates a full audio file using the ElevenLabs API. It then uses whisperX to obtain word-level timestamps and identifies the start and end of the spoken text. Finally, it uses pydub to create a new audio file containing only the audio for the spoken text. \n\n3) \n- The text describes a Python script that uses the ElevenLabs API, whisperX, and pydub to generate and splice audio files.\n- The script takes two strings as input, one for the spoken text and one for the text used to give it emotion.\n- The script generates a full audio file using the ElevenLabs API.\n- The script uses whisperX to obtain word-level timestamps.\n- The script identifies the start and end of the spoken text.\n- The script uses pydub to create a new audio file containing only the audio for the spoken text.\n- The text provides instructions for trying out the script, including installing required libraries and editing the API key in the script. \n- The text notes a dependency conflict due to pyannote-audio requiring an older version of soundfile than the minimum for mp3 support, but provides a workaround using a requirements file with pinned versions."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text does not explicitly mention which endpoints of the ElevenLabs API are used by the Python script. However, based on the description of the script's functionality, it is likely that the `/v1/text-to-speech/{voice_id}` endpoint is used to generate the full audio file, and the `/v1/voices/{voice_id}/samples/{sample_id}/audio` endpoint is used to obtain the audio for the spoken text."
            }
        ]
    },
    {
        "title": "Swift5 API client for ElevenLabsAPI: https://github.com/trissylegs/swift-elevenlabs-io-openapi",
        "text": "About\nSwift 5 elevenlabs.io api generated using openapi generator\n\nResources\n Readme\nStars\n 2 stars\nWatchers\n 2 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nSwift\n98.1%\n \nShell\n1.5%\n \nRuby\n0.4%\n\n\n\n@trissylegs\ntrissylegs swift-elevenlabs-io-openapi\nd16e15f\non Feb 1\nGit stats\n 1 commit\nFiles\nType\nName\nLatest commit message\nCommit time\n.openapi-generator\nswift-elevenlabs-io-openapi\n3 months ago\nElevenLabsAPI/Classes/OpenAPIs\nswift-elevenlabs-io-openapi\n3 months ago\ndocs\nswift-elevenlabs-io-openapi\n3 months ago\n.gitignore\nswift-elevenlabs-io-openapi\n3 months ago\n.openapi-generator-ignore\nswift-elevenlabs-io-openapi\n3 months ago\n.swiftformat\nswift-elevenlabs-io-openapi\n3 months ago\nCartfile\nswift-elevenlabs-io-openapi\n3 months ago\nElevenLabsAPI.podspec\nswift-elevenlabs-io-openapi\n3 months ago\nPackage.swift\nswift-elevenlabs-io-openapi\n3 months ago\nREADME.md\nswift-elevenlabs-io-openapi\n3 months ago\ngenerate.sh\nswift-elevenlabs-io-openapi\n3 months ago\ngit_push.sh\nswift-elevenlabs-io-openapi\n3 months ago\nproject.yml\nswift-elevenlabs-io-openapi\n3 months ago\nREADME.md\nSwift5 API client for ElevenLabsAPI\nThis is the documentation for the ElevenLabs API. You can use this API to use our service programmatically, this is done by using your xi-api-key.\nYou can view your xi-api-key using the 'Profile' tab on https://beta.elevenlabs.io. Our API is experimental so all endpoints are subject to change.\n\nOverview\nThis API client was generated by the OpenAPI Generator project. By using the openapi-spec from a remote server, you can easily generate an API client.\n\nAPI version: 1.0\nPackage version:\nBuild package: org.openapitools.codegen.languages.Swift5ClientCodegen\nInstallation\nCarthage\nRun carthage update\n\nCocoaPods\nRun pod install\n\nDocumentation for API Endpoints\nAll URIs are relative to http://localhost\n\nClass\tMethod\tHTTP request\tDescription\nHistoryAPI\tdeleteHistoryItemV1HistoryHistoryItemIdDelete\tDELETE /v1/history/{history_item_id}\tDelete History Item\nHistoryAPI\tdeleteHistoryItemsV1HistoryDeletePost\tPOST /v1/history/delete\tDelete History Items\nHistoryAPI\tdownloadHistoryItemsV1HistoryDownloadPost\tPOST /v1/history/download\tDownload History Items\nHistoryAPI\tgetAudioFromHistoryItemV1HistoryHistoryItemIdAudioGet\tGET /v1/history/{history_item_id}/audio\tGet Audio From History Item\nHistoryAPI\tgetGeneratedItemsV1HistoryGet\tGET /v1/history\tGet Generated Items\nSamplesAPI\tdeleteSampleV1VoicesVoiceIdSamplesSampleIdDelete\tDELETE /v1/voices/{voice_id}/samples/{sample_id}\tDelete Sample\nSamplesAPI\tgetAudioFromSampleV1VoicesVoiceIdSamplesSampleIdAudioGet\tGET /v1/voices/{voice_id}/samples/{sample_id}/audio\tGet Audio From Sample\nTextToSpeechAPI\ttextToSpeechV1TextToSpeechVoiceIdPost\tPOST /v1/text-to-speech/{voice_id}\tText To Speech\nTextToSpeechAPI\ttextToSpeechV1TextToSpeechVoiceIdStreamPost\tPOST /v1/text-to-speech/{voice_id}/stream\tText To Speech\nUserAPI\tgetUserInfoV1UserGet\tGET /v1/user\tGet User Info\nUserAPI\tgetUserSubscriptionInfoV1UserSubscriptionGet\tGET /v1/user/subscription\tGet User Subscription Info\nVoicesAPI\taddVoiceV1VoicesAddPost\tPOST /v1/voices/add\tAdd Voice\nVoicesAPI\tdeleteVoiceV1VoicesVoiceIdDelete\tDELETE /v1/voices/{voice_id}\tDelete Voice\nVoicesAPI\teditVoiceSettingsV1VoicesVoiceIdSettingsEditPost\tPOST /v1/voices/{voice_id}/settings/edit\tEdit Voice Settings\nVoicesAPI\teditVoiceV1VoicesVoiceIdEditPost\tPOST /v1/voices/{voice_id}/edit\tEdit Voice\nVoicesAPI\tgetDefaultVoiceSettingsV1VoicesSettingsDefaultGet\tGET /v1/voices/settings/default\tGet Default Voice Settings\nVoicesAPI\tgetVoiceSettingsV1VoicesVoiceIdSettingsGet\tGET /v1/voices/{voice_id}/settings\tGet Voice Settings\nVoicesAPI\tgetVoiceV1VoicesVoiceIdGet\tGET /v1/voices/{voice_id}\tGet Voice\nVoicesAPI\tgetVoicesV1VoicesGet\tGET /v1/voices\tGet Voices\nDocumentation For Models\nAddVoiceResponseModel\nBodyDeleteHistoryItemsV1HistoryDeletePost\nBodyDownloadHistoryItemsV1HistoryDownloadPost\nBodyTextToSpeechV1TextToSpeechVoiceIdPost\nBodyTextToSpeechV1TextToSpeechVoiceIdStreamPost\nExtendedSubscriptionResponseModel\nGetHistoryResponseModel\nGetVoicesResponseModel\nHTTPValidationError\nHistoryItemResponseModel\nInvoiceResponseModel\nLocationInner\nSampleResponseModel\nSettings\nSubscriptionResponseModel\nUserResponseModel\nValidationError\nVoiceResponseModel\nVoiceSettingsResponseModel\nDocumentation For Authorization\nAll endpoints do not require authorization.\n\nAuthor",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```Swift5 API client for ElevenLabsAPI: https://github.com/trissylegs/swift-elevenlabs-io-openapi\nAbout\nSwift 5 elevenlabs.io api generated using openapi generator\n\nResources\n Readme\nStars\n 2 stars\nWatchers\n 2 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nSwift\n98.1%\n \nShell\n1.5%\n \nRuby\n0.4%\n\n\n\n@trissylegs\ntrissylegs swift-elevenlabs-io-openapi\nd16e15f\non Feb 1\nGit stats\n 1 commit\nFiles\nType\nName\nLatest commit message\nCommit time\n.openapi-generator\nswift-elevenlabs-io-openapi\n3 months ago\nElevenLabsAPI/Classes/OpenAPIs\nswift-elevenlabs-io-openapi\n3 months ago\ndocs\nswift-elevenlabs-io-openapi\n3 months ago\n.gitignore\nswift-elevenlabs-io-openapi\n3 months ago\n.openapi-generator-ignore\nswift-elevenlabs-io-openapi\n3 months ago\n.swiftformat\nswift-elevenlabs-io-openapi\n3 months ago\nCartfile\nswift-elevenlabs-io-openapi\n3 months ago\nElevenLabsAPI.podspec\nswift-elevenlabs-io-openapi\n3 months ago\nPackage.swift\nswift-elevenlabs-io-openapi\n3 months ago\nREADME.md\nswift-elevenlabs-io-openapi\n3 months ago\ngenerate.sh\nswift-elevenlabs-io-openapi\n3 months ago\ngit_push.sh\nswift-elevenlabs-io-openapi\n3 months ago\nproject.yml\nswift-elevenlabs-io-openapi\n3 months ago\nREADME.md\nSwift5 API client for ElevenLabsAPI\nThis is the documentation for the ElevenLabs API. You can use this API to use our service programmatically, this is done by using your xi-api-key.\nYou can view your xi-api-key using the 'Profile' tab on https://beta.elevenlabs.io. Our API is experimental so all endpoints are subject to change.\n\nOverview\nThis API client was generated by the OpenAPI Generator project. By using the openapi-spec from a remote server, you can easily generate an API client.\n\nAPI version: 1.0\nPackage version:\nBuild package: org.openapitools.codegen.languages.Swift5ClientCodegen\nInstallation\nCarthage\nRun carthage update\n\nCocoaPods\nRun pod install\n\nDocumentation for API Endpoints\nAll URIs are relative to http://localhost\n\nClass\tMethod\tHTTP request\tDescription\nHistoryAPI\tdeleteHistoryItemV1HistoryHistoryItemIdDelete\tDELETE /v1/history/{history_item_id}\tDelete History Item\nHistoryAPI\tdeleteHistoryItemsV1HistoryDeletePost\tPOST /v1/history/delete\tDelete History Items\nHistoryAPI\tdownloadHistoryItemsV1HistoryDownloadPost\tPOST /v1/history/download\tDownload History Items\nHistoryAPI\tgetAudioFromHistoryItemV1HistoryHistoryItemIdAudioGet\tGET /v1/history/{history_item_id}/audio\tGet Audio From History Item\nHistoryAPI\tgetGeneratedItemsV1HistoryGet\tGET /v1/history\tGet Generated Items\nSamplesAPI\tdeleteSampleV1VoicesVoiceIdSamplesSampleIdDelete\tDELETE /v1/voices/{voice_id}/samples/{sample_id}\tDelete Sample\nSamplesAPI\tgetAudioFromSampleV1VoicesVoiceIdSamplesSampleIdAudioGet\tGET /v1/voices/{voice_id}/samples/{sample_id}/audio\tGet Audio From Sample\nTextToSpeechAPI\ttextToSpeechV1TextToSpeechVoiceIdPost\tPOST /v1/text-to-speech/{voice_id}\tText To Speech\nTextToSpeechAPI\ttextToSpeechV1TextToSpeechVoiceIdStreamPost\tPOST /v1/text-to-speech/{voice_id}/stream\tText To Speech\nUserAPI\tgetUserInfoV1UserGet\tGET /v1/user\tGet User Info\nUserAPI\tgetUserSubscriptionInfoV1UserSubscriptionGet\tGET /v1/user/subscription\tGet User Subscription Info\nVoicesAPI\taddVoiceV1VoicesAddPost\tPOST /v1/voices/add\tAdd Voice\nVoicesAPI\tdeleteVoiceV1VoicesVoiceIdDelete\tDELETE /v1/voices/{voice_id}\tDelete Voice\nVoicesAPI\teditVoiceSettingsV1VoicesVoiceIdSettingsEditPost\tPOST /v1/voices/{voice_id}/settings/edit\tEdit Voice Settings\nVoicesAPI\teditVoiceV1VoicesVoiceIdEditPost\tPOST /v1/voices/{voice_id}/edit\tEdit Voice\nVoicesAPI\tgetDefaultVoiceSettingsV1VoicesSettingsDefaultGet\tGET /v1/voices/settings/default\tGet Default Voice Settings\nVoicesAPI\tgetVoiceSettingsV1VoicesVoiceIdSettingsGet\tGET /v1/voices/{voice_id}/settings\tGet Voice Settings\nVoicesAPI\tgetVoiceV1VoicesVoiceIdGet\tGET /v1/voices/{voice_id}\tGet Voice\nVoicesAPI\tgetVoicesV1VoicesGet\tGET /v1/voices\tGet Voices\nDocumentation For Models\nAddVoiceResponseModel\nBodyDeleteHistoryItemsV1HistoryDeletePost\nBodyDownloadHistoryItemsV1HistoryDownloadPost\nBodyTextToSpeechV1TextToSpeechVoiceIdPost\nBodyTextToSpeechV1TextToSpeechVoiceIdStreamPost\nExtendedSubscriptionResponseModel\nGetHistoryResponseModel\nGetVoicesResponseModel\nHTTPValidationError\nHistoryItemResponseModel\nInvoiceResponseModel\nLocationInner\nSampleResponseModel\nSettings\nSubscriptionResponseModel\nUserResponseModel\nValidationError\nVoiceResponseModel\nVoiceSettingsResponseModel\nDocumentation For Authorization\nAll endpoints do not require authorization.\n\nAuthor\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for the ElevenLabs API, which includes various endpoints for accessing different resources such as models, voices, samples, and history. The API is experimental and subject to change.\n\n2) The text is a Swift 5 API client for the ElevenLabs API, generated using the OpenAPI Generator project. It provides an overview of the API and its endpoints, as well as instructions for installation using Carthage or CocoaPods.\n\n3) \n- The text provides an overview of the ElevenLabs API and its endpoints, including HistoryAPI, SamplesAPI, TextToSpeechAPI, UserAPI, and VoicesAPI.\n- The API version is 1.0 and the package version is generated using Swift5ClientCodegen.\n- The text provides instructions for installation using Carthage or CocoaPods.\n- All URIs are relative to http://localhost.\n- The text provides a breakdown of the different classes and methods for accessing the API endpoints, including their HTTP requests and descriptions.\n- The text also provides documentation for the different models and authorizations used in the API.\n- The API is experimental and subject to change."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text provides an overview of all the endpoints used in the ElevenLabs API, including:\n\n- HistoryAPI\n- SamplesAPI\n- TextToSpeechAPI\n- UserAPI\n- VoicesAPI\n\nEach endpoint has its own set of methods for accessing different resources within the API."
            }
        ]
    },
    {
        "title": "Get VoiceID using JS: https://github.com/SagerKudrick/elevenlabs_get_custom_voiceID",
        "text": "About\nquickly get IDs of your custom voices\n\nStars\n 1 star\nWatchers\n 2 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nJavaScript\n100.0%\n\n\n@SagerKudrick\nSagerKudrick Update customvoiceid.js\n\u00e2\u20ac\u00a6\n3942c5b\non Mar 11\nGit stats\n 5 commits\nFiles\nType\nName\nLatest commit message\nCommit time\ncustomvoiceid.js\nUpdate customvoiceid.js\n2 months ago\noptions.json\nCreate options.json",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```Get VoiceID using JS: https://github.com/SagerKudrick/elevenlabs_get_custom_voiceID\nAbout\nquickly get IDs of your custom voices\n\nStars\n 1 star\nWatchers\n 2 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nJavaScript\n100.0%\n\n\n@SagerKudrick\nSagerKudrick Update customvoiceid.js\n\u00e2\u20ac\u00a6\n3942c5b\non Mar 11\nGit stats\n 5 commits\nFiles\nType\nName\nLatest commit message\nCommit time\ncustomvoiceid.js\nUpdate customvoiceid.js\n2 months ago\noptions.json\nCreate options.json\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info is a documentation of the API provided by ElevenLabs. It includes various endpoints for accessing and manipulating different aspects of the platform, such as models, voices, samples, and history. \n\n2) The text is a GitHub repository called \"elevenlabs_get_custom_voiceID\" created by a user named SagerKudrick. It provides a JavaScript file called \"customvoiceid.js\" that allows users to quickly get the IDs of their custom voices on the ElevenLabs platform. \n\n3) \n- The GitHub repository is called \"elevenlabs_get_custom_voiceID\"\n- The repository was created by a user named SagerKudrick\n- The repository provides a JavaScript file called \"customvoiceid.js\"\n- The JavaScript file allows users to quickly get the IDs of their custom voices on the ElevenLabs platform\n- The JavaScript file was updated by SagerKudrick on March 11th\n- The repository has 1 star and 2 watchers\n- The repository has 0 forks\n- There have been 5 commits to the repository\n- The repository has no releases or packages published\n- The JavaScript file is the only file in the repository, but there is also an \"options.json\" file that was created at some point."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text \"elevenlabs_get_custom_voiceID\" and the JavaScript file \"customvoiceid.js\" do not directly use any of the endpoints provided by the ElevenLabs API documentation. However, the JavaScript file may use some of the endpoints internally to retrieve the custom voice IDs. Without further analysis of the code in the JavaScript file, it is not possible to determine which endpoints are used."
            }
        ]
    },
    {
        "title": "A non-official ElevenLabs voice synthesis RESTful client for the Unity Game Engine. https://github.com/RageAgainstThePixel/com.rest.elevenlabs",
        "text": "About\nA non-official Eleven Labs voice synthesis client for Unity (UPM)\n\nTopics\nai unity unity3d ml tts upm voice-synthesis upm-package\nResources\n Readme\nLicense\n MIT license\nStars\n 16 stars\nWatchers\n 3 watching\nForks\n 1 fork\nReport repository\nReleases 10\ncom.rest.elevenlabs 1.3.0\nLatest\n3 hours ago\n+ 9 releases\nSponsor this project\n@StephenHodgson\nStephenHodgson Stephen Hodgson\nLearn more about GitHub Sponsors\nLanguages\nC#\n100.0%\n\n\n\n@StephenHodgson\nStephenHodgson com.rest.elevenlabs 1.3.0 (#12)\n\u2026\n0a51487\n3 hours ago\nGit stats\n 11 commits\nFiles\nType\nName\nLatest commit message\nCommit time\n.github\nInitial commit\n3 months ago\nElevenLabs\ncom.rest.elevenlabs 1.3.0 (#12)\n3 hours ago\n.gitignore\ncom.rest.elevenlabs 1.0.0 (#4)\n2 months ago\nLICENSE.md\ncom.rest.elevenlabs 1.0.0-preview.1 (#1)\n2 months ago\nREADME.md\ncom.rest.elevenlabs 1.2.1\n20 hours ago\nREADME.md\ncom.rest.elevenlabs\nDiscord openupm\n\nA non-official ElevenLabs voice synthesis RESTful client for the Unity Game Engine.\n\nBased on ElevenLabs-DotNet\n\nI am not affiliated with ElevenLabs and an account with api access is required.\n\nAll copyrights, trademarks, logos, and assets are the property of their respective owners.\n\nInstalling\nVia Unity Package Manager and OpenUPM\nOpen your Unity project settings\nSelect the Package Manager scoped-registries\nAdd the OpenUPM package registry:\nName: OpenUPM\nURL: https://package.openupm.com\nScope(s):\ncom.rest.elevenlabs\ncom.utilities\nOpen the Unity Package Manager window\nChange the Registry from Unity to My Registries\nAdd the ElevenLabs package\nVia Unity Package Manager and Git url\nOpen your Unity Package Manager\nAdd package from git url: https://github.com/RageAgainstThePixel/com.rest.elevenlabs.git#upm\nNote: this repo has dependencies on other repositories! You are responsible for adding these on your own.\n\ncom.utilities.async\ncom.utilities.audio\ncom.utilities.rest\nDocumentation\nTable of Contents\nAuthentication\nAPI Proxy \ud83c\udd95\nEditor Dashboard\nSpeech Synthesis\nVoice Lab\nVoice Designer\nVoice Cloning\nHistory\nText to Speech\nStream Text to Speech\nVoices\nGet All Voices\nGet Default Voice Settings\nGet Voice\nEdit Voice Settings\nAdd Voice\nEdit Voice\nDelete Voice\nSamples\nGet Voice Sample\nDelete Voice Sample\nHistory\nGet History\nGet History Audio\nDownload All History\nDelete History Item\nUser\nGet User Info\nGet Subscription Info\nAuthentication\nThere are 4 ways to provide your API keys, in order of precedence:\n\nPass keys directly with constructor\nUnity Scriptable Object\nLoad key from configuration file\nUse System Environment Variables\nPass keys directly with constructor\nvar api = new ElevenLabsClient(\"yourApiKey\");\nOr create a ElevenLabsAuthentication object manually\n\nvar api = new ElevenLabsClient(new ElevenLabsAuthentication(\"yourApiKey\"));\nUnity Scriptable Object\nYou can save the key directly into a scriptable object that is located in the Assets/Resources folder.\n\nYou can create a new one by using the context menu of the project pane and creating a new ElevenLabsConfigurationSettings scriptable object.\n\nCreate new ElevenLabsConfigurationSettings\n\nLoad key from configuration file\nAttempts to load api keys from a configuration file, by default .elevenlabs in the current directory, optionally traversing up the directory tree or in the user's home directory.\n\nTo create a configuration file, create a new text file named .elevenlabs and containing the line:\n\nJson format\n{\n  \"apiKey\": \"yourApiKey\",\n}\nYou can also load the file directly with known path by calling a static method in Authentication:\n\nvar api = new ElevenLabsClient(ElevenLabsAuthentication.LoadFromDirectory(\"your/path/to/.elevenlabs\"));;\nUse System Environment Variables\nUse your system's environment variables specify an api key to use.\n\nUse ELEVEN_LABS_API_KEY for your api key.\nvar api = new ElevenLabsClient(ElevenLabsAuthentication.LoadFromEnv());\nAPI Proxy\nNuGet version (ElevenLabs-DotNet-Proxy)\n\nUsing either the ElevenLabs-DotNet or com.rest.elevenlabs packages directly in your front-end app may expose your API keys and other sensitive information. To mitigate this risk, it is recommended to set up an intermediate API that makes requests to ElevenLabs on behalf of your front-end app. This library can be utilized for both front-end and intermediary host configurations, ensuring secure communication with the ElevenLabs API.\n\nFront End Example\nIn the front end example, you will need to securely authenticate your users using your preferred OAuth provider. Once the user is authenticated, exchange your custom auth token with your API key on the backend.\n\nFollow these steps:\n\nSetup a new project using either the ElevenLabs-DotNet or com.rest.elevenlabs packages.\nAuthenticate users with your OAuth provider.\nAfter successful authentication, create a new ElevenLabsAuthentication object and pass in the custom token.\nCreate a new ElevenLabsClientSettings object and specify the domain where your intermediate API is located.\nPass your new auth and settings objects to the ElevenLabsClient constructor when you create the client instance.\nHere's an example of how to set up the front end:\n\nvar authToken = await LoginAsync();\nvar auth = new ElevenLabsAuthentication(authToken);\nvar settings = new ElevenLabsClientSettings(domain: \"api.your-custom-domain.com\");\nvar api = new ElevenLabsClient(auth, settings);\nThis setup allows your front end application to securely communicate with your backend that will be using the ElevenLabs-DotNet-Proxy, which then forwards requests to the ElevenLabs API. This ensures that your ElevenLabs API keys and other sensitive information remain secure throughout the process.\n\nBack End Example\nIn this example, we demonstrate how to set up and use ElevenLabsProxyStartup in a new ASP.NET Core web app. The proxy server will handle authentication and forward requests to the ElevenLabs API, ensuring that your API keys and other sensitive information remain secure.\n\nCreate a new ASP.NET Core minimal web API project.\nAdd the ElevenLabs-DotNet nuget package to your project.\nPowershell install: Install-Package ElevenLabs-DotNet-Proxy\nManually editing .csproj: <PackageReference Include=\"ElevenLabs-DotNet-Proxy\" />\nCreate a new class that inherits from AbstractAuthenticationFilter and override the ValidateAuthentication method. This will implement the IAuthenticationFilter that you will use to check user session token against your internal server.\nIn Program.cs, create a new proxy web application by calling ElevenLabsProxyStartup.CreateDefaultHost method, passing your custom AuthenticationFilter as a type argument.\nCreate ElevenLabsAuthentication and ElevenLabsClientSettings as you would normally with your API keys, org id, or Azure settings.\npublic partial class Program\n{\n    private class AuthenticationFilter : AbstractAuthenticationFilter\n    {\n        public override void ValidateAuthentication(IHeaderDictionary request)\n        {\n            // You will need to implement your own class to properly test\n            // custom issued tokens you've setup for your end users.\n            if (!request[\"xi-api-key\"].ToString().Contains(userToken))\n            {\n                throw new AuthenticationException(\"User is not authorized\");\n            }\n        }\n    }\n\n    public static void Main(string[] args)\n    {\n        var client = new ElevenLabsClient();\n        var proxy = ElevenLabsProxyStartup.CreateDefaultHost<AuthenticationFilter>(args, client);\n        proxy.Run();\n    }\n}\nOnce you have set up your proxy server, your end users can now make authenticated requests to your proxy api instead of directly to the ElevenLabs API. The proxy server will handle authentication and forward requests to the ElevenLabs API, ensuring that your API keys and other sensitive information remain secure.\n\nEditor Dashboard\nYou can perform all of the same actions from the ElevenLabs website, in the Editor using the ElevenLabs Dashboard! dashboard\n\nSpeech Synthesis\nJust like in the ElevenLabs website, you can synthesize new audio clips using available voices. This tool makes it even more handy as the clips are automatically downloaded and imported into your project, ready for you to use!\n\nSpeech Synthesis\n\nVoice Lab\nJust like in the ElevenLabs website, you can manage all your voices directly in the editor.\n\nVoice Lab\n\nVoice Designer\nSelecting Create New Voice will display a popup where you can design entirely new voices using ElevenLabs generative models.\n\nVoice Designer\n\nVoice Cloning\nAdditionally, you're also able to clone a voice from sample recordings.\n\nVoice Cloning\n\nHistory\nYou also have access to the full list of all your generated samples, ready for download.\n\nHistory\n\nText to Speech\nConvert text to speech.\n\nvar api = new ElevenLabsClient();\nvar text = \"The quick brown fox jumps over the lazy dog.\";\nvar voice = (await api.VoicesEndpoint.GetAllVoicesAsync()).FirstOrDefault();\nvar defaultVoiceSettings = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nvar (clipPath, audioClip) = await api.TextToSpeechEndpoint.TextToSpeechAsync(text, voice, defaultVoiceSettings);\nDebug.Log(clipPath);\nNote, only a single audio clip is created per text string. If you'd like to get different variations of the audio, you'll need to pass in deleteCachedFile: true.\n\nStream Text to Speech\n\u26a0\ufe0f WARNING \u203c\ufe0f This feature is experimental, and may not work as expected! Unity doesn't seem to respect streaming when setting DownloadHandlerAudioClip.streamAudio to true.\n\nvar api = new ElevenLabsClient();\nvar text = \"The quick brown fox jumps over the lazy dog.\";\nvar voice = (await api.VoicesEndpoint.GetAllVoicesAsync()).FirstOrDefault();\nvar defaultVoiceSettings = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nvar (clipPath, audioClip) = await api.TextToSpeechEndpoint.TextToSpeechAsync(\n    text,\n    voice,\n    clip =>\n    {\n        // Event raised as soon as the clip has loaded enough data to play.\n        // May not provide or play full clip until Unity bug is addressed.\n        audioSource.PlayOneShot(clip);\n    },\n    defaultVoiceSettings);\nDebug.Log(clipPath);\nVoices\nAccess to voices created either by the user or ElevenLabs.\n\nGet All Voices\nGets a list of all available voices.\n\nvar api = new ElevenLabsClient();\nvar allVoices = await api.VoicesEndpoint.GetAllVoicesAsync();\n\nforeach (var voice in allVoices)\n{\n    Debug.Log($\"{voice.Id} | {voice.Name} | similarity boost: {voice.Settings?.SimilarityBoost} | stability: {voice.Settings?.Stability}\");\n}\nGet Default Voice Settings\nGets the global default voice settings.\n\nvar api = new ElevenLabsClient();\nvar result = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nDebug.Log($\"stability: {result.Stability} | similarity boost: {result.SimilarityBoost}\");\nGet Voice\nvar api = new ElevenLabsClient();\nvar voice = await api.VoicesEndpoint.GetVoiceAsync(\"voiceId\");\nDebug.Log($\"{voice.Id} | {voice.Name} | {voice.PreviewUrl}\");\nEdit Voice Settings\nEdit the settings for a specific voice.\n\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.EditVoiceSettingsAsync(voice, new VoiceSettings(0.7f, 0.7f));\nDebug.Log($\"Was successful? {success}\");\nAdd Voice\nvar api = new ElevenLabsClient();\nvar labels = new Dictionary<string, string>\n{\n    { \"accent\", \"american\" }\n};\nvar audioSamplePaths = new List<string>();\nvar voice = await api.VoicesEndpoint.AddVoiceAsync(\"Voice Name\", audioSamplePaths, labels);\nEdit Voice\nvar api = new ElevenLabsClient();\nvar labels = new Dictionary<string, string>\n{\n    { \"age\", \"young\" }\n};\nvar audioSamplePaths = new List<string>();\nvar success = await api.VoicesEndpoint.EditVoiceAsync(voice, audioSamplePaths, labels);\nDebug.Log($\"Was successful? {success}\");\nDelete Voice\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.DeleteVoiceAsync(voiceId);\nDebug.Log($\"Was successful? {success}\");\nSamples\nAccess to your samples, created by you when cloning voices.\n\nGet Voice Sample\nvar api = new ElevenLabsClient();\nvar audioClip = await api.VoicesEndpoint.GetVoiceSampleAsync(voiceId, sampleId);\nDelete Voice Sample\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.DeleteVoiceSampleAsync(voiceId, sampleId);\nDebug.Log($\"Was successful? {success}\");\nHistory\nAccess to your previously synthesized audio clips including its metadata.\n\nGet History\nvar api = new ElevenLabsClient();\nvar historyItems = await api.HistoryEndpoint.GetHistoryAsync();\n\nforeach (var historyItem in historyItems.OrderBy(historyItem => historyItem.Date))\n{\n    Debug.Log($\"{historyItem.State} {historyItem.Date} | {historyItem.Id} | {historyItem.Text.Length} | {historyItem.Text}\");\n}\nGet History Audio\nvar api = new ElevenLabsClient();\nvar audioClip = await api.HistoryEndpoint.GetHistoryAudioAsync(historyItem);\nDownload All History\nvar api = new ElevenLabsClient();\nvar success = await api.HistoryEndpoint.DownloadHistoryItemsAsync();\nDelete History Item\nvar api = new ElevenLabsClient();\nvar result = await api.HistoryEndpoint.DeleteHistoryItemAsync(historyItem);\nDebug.Log($\"Was successful? {success}\");\nUser\nAccess to your user Information and subscription status.\n\nGet User Info\nGets information about your user account with ElevenLabs.\n\nvar api = new ElevenLabsClient();\nvar userInfo = await api.UserEndpoint.GetUserInfoAsync();\nGet Subscription Info\nGets information about your subscription with ElevenLabs.\n\nvar api = new ElevenLabsClient();\nvar subscriptionInfo = await api.UserEndpoint.GetSubscriptionInfoAsync();",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```A non-official ElevenLabs voice synthesis RESTful client for the Unity Game Engine. https://github.com/RageAgainstThePixel/com.rest.elevenlabs\nAbout\nA non-official Eleven Labs voice synthesis client for Unity (UPM)\n\nTopics\nai unity unity3d ml tts upm voice-synthesis upm-package\nResources\n Readme\nLicense\n MIT license\nStars\n 16 stars\nWatchers\n 3 watching\nForks\n 1 fork\nReport repository\nReleases 10\ncom.rest.elevenlabs 1.3.0\nLatest\n3 hours ago\n+ 9 releases\nSponsor this project\n@StephenHodgson\nStephenHodgson Stephen Hodgson\nLearn more about GitHub Sponsors\nLanguages\nC#\n100.0%\n\n\n\n@StephenHodgson\nStephenHodgson com.rest.elevenlabs 1.3.0 (#12)\n\u2026\n0a51487\n3 hours ago\nGit stats\n 11 commits\nFiles\nType\nName\nLatest commit message\nCommit time\n.github\nInitial commit\n3 months ago\nElevenLabs\ncom.rest.elevenlabs 1.3.0 (#12)\n3 hours ago\n.gitignore\ncom.rest.elevenlabs 1.0.0 (#4)\n2 months ago\nLICENSE.md\ncom.rest.elevenlabs 1.0.0-preview.1 (#1)\n2 months ago\nREADME.md\ncom.rest.elevenlabs 1.2.1\n20 hours ago\nREADME.md\ncom.rest.elevenlabs\nDiscord openupm\n\nA non-official ElevenLabs voice synthesis RESTful client for the Unity Game Engine.\n\nBased on ElevenLabs-DotNet\n\nI am not affiliated with ElevenLabs and an account with api access is required.\n\nAll copyrights, trademarks, logos, and assets are the property of their respective owners.\n\nInstalling\nVia Unity Package Manager and OpenUPM\nOpen your Unity project settings\nSelect the Package Manager scoped-registries\nAdd the OpenUPM package registry:\nName: OpenUPM\nURL: https://package.openupm.com\nScope(s):\ncom.rest.elevenlabs\ncom.utilities\nOpen the Unity Package Manager window\nChange the Registry from Unity to My Registries\nAdd the ElevenLabs package\nVia Unity Package Manager and Git url\nOpen your Unity Package Manager\nAdd package from git url: https://github.com/RageAgainstThePixel/com.rest.elevenlabs.git#upm\nNote: this repo has dependencies on other repositories! You are responsible for adding these on your own.\n\ncom.utilities.async\ncom.utilities.audio\ncom.utilities.rest\nDocumentation\nTable of Contents\nAuthentication\nAPI Proxy \ud83c\udd95\nEditor Dashboard\nSpeech Synthesis\nVoice Lab\nVoice Designer\nVoice Cloning\nHistory\nText to Speech\nStream Text to Speech\nVoices\nGet All Voices\nGet Default Voice Settings\nGet Voice\nEdit Voice Settings\nAdd Voice\nEdit Voice\nDelete Voice\nSamples\nGet Voice Sample\nDelete Voice Sample\nHistory\nGet History\nGet History Audio\nDownload All History\nDelete History Item\nUser\nGet User Info\nGet Subscription Info\nAuthentication\nThere are 4 ways to provide your API keys, in order of precedence:\n\nPass keys directly with constructor\nUnity Scriptable Object\nLoad key from configuration file\nUse System Environment Variables\nPass keys directly with constructor\nvar api = new ElevenLabsClient(\"yourApiKey\");\nOr create a ElevenLabsAuthentication object manually\n\nvar api = new ElevenLabsClient(new ElevenLabsAuthentication(\"yourApiKey\"));\nUnity Scriptable Object\nYou can save the key directly into a scriptable object that is located in the Assets/Resources folder.\n\nYou can create a new one by using the context menu of the project pane and creating a new ElevenLabsConfigurationSettings scriptable object.\n\nCreate new ElevenLabsConfigurationSettings\n\nLoad key from configuration file\nAttempts to load api keys from a configuration file, by default .elevenlabs in the current directory, optionally traversing up the directory tree or in the user's home directory.\n\nTo create a configuration file, create a new text file named .elevenlabs and containing the line:\n\nJson format\n{\n  \"apiKey\": \"yourApiKey\",\n}\nYou can also load the file directly with known path by calling a static method in Authentication:\n\nvar api = new ElevenLabsClient(ElevenLabsAuthentication.LoadFromDirectory(\"your/path/to/.elevenlabs\"));;\nUse System Environment Variables\nUse your system's environment variables specify an api key to use.\n\nUse ELEVEN_LABS_API_KEY for your api key.\nvar api = new ElevenLabsClient(ElevenLabsAuthentication.LoadFromEnv());\nAPI Proxy\nNuGet version (ElevenLabs-DotNet-Proxy)\n\nUsing either the ElevenLabs-DotNet or com.rest.elevenlabs packages directly in your front-end app may expose your API keys and other sensitive information. To mitigate this risk, it is recommended to set up an intermediate API that makes requests to ElevenLabs on behalf of your front-end app. This library can be utilized for both front-end and intermediary host configurations, ensuring secure communication with the ElevenLabs API.\n\nFront End Example\nIn the front end example, you will need to securely authenticate your users using your preferred OAuth provider. Once the user is authenticated, exchange your custom auth token with your API key on the backend.\n\nFollow these steps:\n\nSetup a new project using either the ElevenLabs-DotNet or com.rest.elevenlabs packages.\nAuthenticate users with your OAuth provider.\nAfter successful authentication, create a new ElevenLabsAuthentication object and pass in the custom token.\nCreate a new ElevenLabsClientSettings object and specify the domain where your intermediate API is located.\nPass your new auth and settings objects to the ElevenLabsClient constructor when you create the client instance.\nHere's an example of how to set up the front end:\n\nvar authToken = await LoginAsync();\nvar auth = new ElevenLabsAuthentication(authToken);\nvar settings = new ElevenLabsClientSettings(domain: \"api.your-custom-domain.com\");\nvar api = new ElevenLabsClient(auth, settings);\nThis setup allows your front end application to securely communicate with your backend that will be using the ElevenLabs-DotNet-Proxy, which then forwards requests to the ElevenLabs API. This ensures that your ElevenLabs API keys and other sensitive information remain secure throughout the process.\n\nBack End Example\nIn this example, we demonstrate how to set up and use ElevenLabsProxyStartup in a new ASP.NET Core web app. The proxy server will handle authentication and forward requests to the ElevenLabs API, ensuring that your API keys and other sensitive information remain secure.\n\nCreate a new ASP.NET Core minimal web API project.\nAdd the ElevenLabs-DotNet nuget package to your project.\nPowershell install: Install-Package ElevenLabs-DotNet-Proxy\nManually editing .csproj: <PackageReference Include=\"ElevenLabs-DotNet-Proxy\" />\nCreate a new class that inherits from AbstractAuthenticationFilter and override the ValidateAuthentication method. This will implement the IAuthenticationFilter that you will use to check user session token against your internal server.\nIn Program.cs, create a new proxy web application by calling ElevenLabsProxyStartup.CreateDefaultHost method, passing your custom AuthenticationFilter as a type argument.\nCreate ElevenLabsAuthentication and ElevenLabsClientSettings as you would normally with your API keys, org id, or Azure settings.\npublic partial class Program\n{\n    private class AuthenticationFilter : AbstractAuthenticationFilter\n    {\n        public override void ValidateAuthentication(IHeaderDictionary request)\n        {\n            // You will need to implement your own class to properly test\n            // custom issued tokens you've setup for your end users.\n            if (!request[\"xi-api-key\"].ToString().Contains(userToken))\n            {\n                throw new AuthenticationException(\"User is not authorized\");\n            }\n        }\n    }\n\n    public static void Main(string[] args)\n    {\n        var client = new ElevenLabsClient();\n        var proxy = ElevenLabsProxyStartup.CreateDefaultHost<AuthenticationFilter>(args, client);\n        proxy.Run();\n    }\n}\nOnce you have set up your proxy server, your end users can now make authenticated requests to your proxy api instead of directly to the ElevenLabs API. The proxy server will handle authentication and forward requests to the ElevenLabs API, ensuring that your API keys and other sensitive information remain secure.\n\nEditor Dashboard\nYou can perform all of the same actions from the ElevenLabs website, in the Editor using the ElevenLabs Dashboard! dashboard\n\nSpeech Synthesis\nJust like in the ElevenLabs website, you can synthesize new audio clips using available voices. This tool makes it even more handy as the clips are automatically downloaded and imported into your project, ready for you to use!\n\nSpeech Synthesis\n\nVoice Lab\nJust like in the ElevenLabs website, you can manage all your voices directly in the editor.\n\nVoice Lab\n\nVoice Designer\nSelecting Create New Voice will display a popup where you can design entirely new voices using ElevenLabs generative models.\n\nVoice Designer\n\nVoice Cloning\nAdditionally, you're also able to clone a voice from sample recordings.\n\nVoice Cloning\n\nHistory\nYou also have access to the full list of all your generated samples, ready for download.\n\nHistory\n\nText to Speech\nConvert text to speech.\n\nvar api = new ElevenLabsClient();\nvar text = \"The quick brown fox jumps over the lazy dog.\";\nvar voice = (await api.VoicesEndpoint.GetAllVoicesAsync()).FirstOrDefault();\nvar defaultVoiceSettings = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nvar (clipPath, audioClip) = await api.TextToSpeechEndpoint.TextToSpeechAsync(text, voice, defaultVoiceSettings);\nDebug.Log(clipPath);\nNote, only a single audio clip is created per text string. If you'd like to get different variations of the audio, you'll need to pass in deleteCachedFile: true.\n\nStream Text to Speech\n\u26a0\ufe0f WARNING \u203c\ufe0f This feature is experimental, and may not work as expected! Unity doesn't seem to respect streaming when setting DownloadHandlerAudioClip.streamAudio to true.\n\nvar api = new ElevenLabsClient();\nvar text = \"The quick brown fox jumps over the lazy dog.\";\nvar voice = (await api.VoicesEndpoint.GetAllVoicesAsync()).FirstOrDefault();\nvar defaultVoiceSettings = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nvar (clipPath, audioClip) = await api.TextToSpeechEndpoint.TextToSpeechAsync(\n    text,\n    voice,\n    clip =>\n    {\n        // Event raised as soon as the clip has loaded enough data to play.\n        // May not provide or play full clip until Unity bug is addressed.\n        audioSource.PlayOneShot(clip);\n    },\n    defaultVoiceSettings);\nDebug.Log(clipPath);\nVoices\nAccess to voices created either by the user or ElevenLabs.\n\nGet All Voices\nGets a list of all available voices.\n\nvar api = new ElevenLabsClient();\nvar allVoices = await api.VoicesEndpoint.GetAllVoicesAsync();\n\nforeach (var voice in allVoices)\n{\n    Debug.Log($\"{voice.Id} | {voice.Name} | similarity boost: {voice.Settings?.SimilarityBoost} | stability: {voice.Settings?.Stability}\");\n}\nGet Default Voice Settings\nGets the global default voice settings.\n\nvar api = new ElevenLabsClient();\nvar result = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nDebug.Log($\"stability: {result.Stability} | similarity boost: {result.SimilarityBoost}\");\nGet Voice\nvar api = new ElevenLabsClient();\nvar voice = await api.VoicesEndpoint.GetVoiceAsync(\"voiceId\");\nDebug.Log($\"{voice.Id} | {voice.Name} | {voice.PreviewUrl}\");\nEdit Voice Settings\nEdit the settings for a specific voice.\n\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.EditVoiceSettingsAsync(voice, new VoiceSettings(0.7f, 0.7f));\nDebug.Log($\"Was successful? {success}\");\nAdd Voice\nvar api = new ElevenLabsClient();\nvar labels = new Dictionary<string, string>\n{\n    { \"accent\", \"american\" }\n};\nvar audioSamplePaths = new List<string>();\nvar voice = await api.VoicesEndpoint.AddVoiceAsync(\"Voice Name\", audioSamplePaths, labels);\nEdit Voice\nvar api = new ElevenLabsClient();\nvar labels = new Dictionary<string, string>\n{\n    { \"age\", \"young\" }\n};\nvar audioSamplePaths = new List<string>();\nvar success = await api.VoicesEndpoint.EditVoiceAsync(voice, audioSamplePaths, labels);\nDebug.Log($\"Was successful? {success}\");\nDelete Voice\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.DeleteVoiceAsync(voiceId);\nDebug.Log($\"Was successful? {success}\");\nSamples\nAccess to your samples, created by you when cloning voices.\n\nGet Voice Sample\nvar api = new ElevenLabsClient();\nvar audioClip = await api.VoicesEndpoint.GetVoiceSampleAsync(voiceId, sampleId);\nDelete Voice Sample\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.DeleteVoiceSampleAsync(voiceId, sampleId);\nDebug.Log($\"Was successful? {success}\");\nHistory\nAccess to your previously synthesized audio clips including its metadata.\n\nGet History\nvar api = new ElevenLabsClient();\nvar historyItems = await api.HistoryEndpoint.GetHistoryAsync();\n\nforeach (var historyItem in historyItems.OrderBy(historyItem => historyItem.Date))\n{\n    Debug.Log($\"{historyItem.State} {historyItem.Date} | {historyItem.Id} | {historyItem.Text.Length} | {historyItem.Text}\");\n}\nGet History Audio\nvar api = new ElevenLabsClient();\nvar audioClip = await api.HistoryEndpoint.GetHistoryAudioAsync(historyItem);\nDownload All History\nvar api = new ElevenLabsClient();\nvar success = await api.HistoryEndpoint.DownloadHistoryItemsAsync();\nDelete History Item\nvar api = new ElevenLabsClient();\nvar result = await api.HistoryEndpoint.DeleteHistoryItemAsync(historyItem);\nDebug.Log($\"Was successful? {success}\");\nUser\nAccess to your user Information and subscription status.\n\nGet User Info\nGets information about your user account with ElevenLabs.\n\nvar api = new ElevenLabsClient();\nvar userInfo = await api.UserEndpoint.GetUserInfoAsync();\nGet Subscription Info\nGets information about your subscription with ElevenLabs.\n\nvar api = new ElevenLabsClient();\nvar subscriptionInfo = await api.UserEndpoint.GetSubscriptionInfoAsync();\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs' API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information.\n2) The text is a description of a non-official ElevenLabs voice synthesis RESTful client for the Unity Game Engine, available on GitHub. It includes information on installation, authentication, API proxy, editor dashboard, speech synthesis, voice lab, voice designer, voice cloning, history, and user information.\n3) \n- The text describes a non-official ElevenLabs voice synthesis RESTful client for the Unity Game Engine, available on GitHub.\n- The client is based on ElevenLabs-DotNet and requires an account with API access.\n- The client includes features such as speech synthesis, voice lab, voice designer, voice cloning, history, and user information.\n- The text provides instructions for installation via Unity Package Manager and OpenUPM or Git URL.\n- The text explains four ways to provide API keys for authentication, in order of precedence.\n- The text describes an API proxy for mitigating security risks when using ElevenLabs packages directly in front-end apps.\n- The text provides examples of how to set up and use the API proxy in front-end and back-end configurations.\n- The text describes the editor dashboard, which allows users to perform all actions available on the ElevenLabs website.\n- The text provides examples of how to use the client to convert text to speech, access voices and voice settings, manage samples and history, and access user information."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text mentions several endpoints from ElevenLabs' API, including:\n\n- POST /v1/text-to-speech/{voice_id}: used for text-to-speech synthesis\n- GET /v1/models: used for accessing models\n- GET /v1/voices: used for accessing voices\n- GET /v1/voices/settings/default: used for accessing default voice settings\n- GET /v1/voices/{voice_id}/settings: used for accessing voice settings\n- GET /v1/voices/{voice_id}: used for accessing a specific voice\n- POST /v1/voices/{voice_id}/settings/edit: used for editing voice settings\n- POST /v1/voices/add: used for adding a new voice\n- POST /v1/voices/{voice_id}/edit: used for editing a voice\n- DELETE /v1/voices/{voice_id}: used for deleting a voice\n- DELETE /v1/voices/{voice_id}/samples/{sample_id}: used for deleting a voice sample\n- GET /v1/history: used for accessing history items\n- GET /v1/history"
            }
        ]
    },
    {
        "title": "A non-official Eleven Labs voice synthesis RESTful client for .Net https://github.com/RageAgainstThePixel/ElevenLabs-DotNet ",
        "text": "About\nA Non-Official ElevenLabs RESTful API Client for dotnet\n\nTopics\nai dotnet ml tts speech-synthesis voice-synthesis tts-api eleven-labs\nResources\n Readme\nLicense\n MIT license\nStars\n 11 stars\nWatchers\n 1 watching\nForks\n 1 fork\nReport repository\nReleases 10\nElevenLabs 1.3.0\nLatest\n3 hours ago\n+ 9 releases\nSponsor this project\n@StephenHodgson\nStephenHodgson Stephen Hodgson\nLearn more about GitHub Sponsors\nLanguages\nC#\n100.0%\n\n\n\n@StephenHodgson\nStephenHodgson ElevenLabs 1.3.0 (#14)\n\u2026\n5f7f729\n3 hours ago\nGit stats\n 15 commits\nFiles\nType\nName\nLatest commit message\nCommit time\n.github\nElevenLabs-DotNet 1.2.0 (#13)\n20 hours ago\nElevenLabs-DotNet-Proxy\nElevenLabs-DotNet 1.1.1 (#12)\n2 months ago\nElevenLabs-DotNet-Tests-Proxy\nElevenLabs-DotNet 1.2.0 (#13)\n20 hours ago\nElevenLabs-DotNet-Tests\nElevenLabs 1.3.0 (#14)\n3 hours ago\nElevenLabs-DotNet\nElevenLabs 1.3.0 (#14)\n3 hours ago\n.gitattributes\nElevenLabs-DotNet 1.0.0 (#1)\n2 months ago\n.gitignore\nElevenLabs-DotNet 1.0.0 (#1)\n2 months ago\nElevenLabs-DotNet.sln\nElevenLabs-DotNet 1.1.0 (#10)\n2 months ago\nLICENSE\nInitial commit\n2 months ago\nREADME.md\nElevenLabs-DotNet 1.2.1\n20 hours ago\nREADME.md\nElevenLabs-DotNet\nDiscord NuGet version (ElevenLabs-DotNet) NuGet version (ElevenLabs-DotNet-Proxy) Nuget Publish\n\nA non-official Eleven Labs voice synthesis RESTful client.\n\nI am not affiliated with Eleven Labs and an account with api access is required.\n\nAll copyrights, trademarks, logos, and assets are the property of their respective owners.\n\nGetting started\nInstall from NuGet\nInstall package ElevenLabs from Nuget. Here's how via command line:\n\nInstall-Package ElevenLabs-DotNet\nLooking to use ElevenLabs in the Unity Game Engine? Check out our unity package on OpenUPM:\n\nopenupm\n\nDocumentation\nTable of Contents\nAuthentication\nAPI Proxy \ud83c\udd95\nText to Speech\nVoices\nGet All Voices\nGet Default Voice Settings\nGet Voice\nEdit Voice Settings\nAdd Voice\nEdit Voice\nDelete Voice\nSamples\nGet Voice Sample\nDelete Voice Sample\nHistory\nGet History\nGet History Audio\nDownload All History\nDelete History Item\nUser\nGet User Info\nGet Subscription Info\nAuthentication\nThere are 3 ways to provide your API keys, in order of precedence:\n\nPass keys directly with constructor\nLoad key from configuration file\nUse System Environment Variables\nPass keys directly with constructor\nvar api = new ElevenLabsClient(\"yourApiKey\");\nOr create a ElevenLabsAuthentication object manually\n\nvar api = new ElevenLabsClient(new ElevenLabsAuthentication(\"yourApiKey\"));\nLoad key from configuration file\nAttempts to load api keys from a configuration file, by default .elevenlabs in the current directory, optionally traversing up the directory tree or in the user's home directory.\n\nTo create a configuration file, create a new text file named .elevenlabs and containing the line:\n\nJson format\n{\n  \"apiKey\": \"yourApiKey\",\n}\nYou can also load the file directly with known path by calling a static method in Authentication:\n\nvar api = new ElevenLabsClient(ElevenLabsAuthentication.LoadFromDirectory(\"your/path/to/.elevenlabs\"));;\nUse System Environment Variables\nUse your system's environment variables specify an api key to use.\n\nUse ELEVEN_LABS_API_KEY for your api key.\nvar api = new ElevenLabsClient(ElevenLabsAuthentication.LoadFromEnv());\nAPI Proxy\nNuGet version (ElevenLabs-DotNet-Proxy)\n\nUsing either the ElevenLabs-DotNet or com.rest.elevenlabs packages directly in your front-end app may expose your API keys and other sensitive information. To mitigate this risk, it is recommended to set up an intermediate API that makes requests to ElevenLabs on behalf of your front-end app. This library can be utilized for both front-end and intermediary host configurations, ensuring secure communication with the ElevenLabs API.\n\nFront End Example\nIn the front end example, you will need to securely authenticate your users using your preferred OAuth provider. Once the user is authenticated, exchange your custom auth token with your API key on the backend.\n\nFollow these steps:\n\nSetup a new project using either the ElevenLabs-DotNet or com.rest.elevenlabs packages.\nAuthenticate users with your OAuth provider.\nAfter successful authentication, create a new ElevenLabsAuthentication object and pass in the custom token.\nCreate a new ElevenLabsClientSettings object and specify the domain where your intermediate API is located.\nPass your new auth and settings objects to the ElevenLabsClient constructor when you create the client instance.\nHere's an example of how to set up the front end:\n\nvar authToken = await LoginAsync();\nvar auth = new ElevenLabsAuthentication(authToken);\nvar settings = new ElevenLabsClientSettings(domain: \"api.your-custom-domain.com\");\nvar api = new ElevenLabsClient(auth, settings);\nThis setup allows your front end application to securely communicate with your backend that will be using the ElevenLabs-DotNet-Proxy, which then forwards requests to the ElevenLabs API. This ensures that your ElevenLabs API keys and other sensitive information remain secure throughout the process.\n\nBack End Example\nIn this example, we demonstrate how to set up and use ElevenLabsProxyStartup in a new ASP.NET Core web app. The proxy server will handle authentication and forward requests to the ElevenLabs API, ensuring that your API keys and other sensitive information remain secure.\n\nCreate a new ASP.NET Core minimal web API project.\nAdd the ElevenLabs-DotNet nuget package to your project.\nPowershell install: Install-Package ElevenLabs-DotNet-Proxy\nManually editing .csproj: <PackageReference Include=\"ElevenLabs-DotNet-Proxy\" />\nCreate a new class that inherits from AbstractAuthenticationFilter and override the ValidateAuthentication method. This will implement the IAuthenticationFilter that you will use to check user session token against your internal server.\nIn Program.cs, create a new proxy web application by calling ElevenLabsProxyStartup.CreateDefaultHost method, passing your custom AuthenticationFilter as a type argument.\nCreate ElevenLabsAuthentication and ElevenLabsClientSettings as you would normally with your API keys, org id, or Azure settings.\npublic partial class Program\n{\n    private class AuthenticationFilter : AbstractAuthenticationFilter\n    {\n        public override void ValidateAuthentication(IHeaderDictionary request)\n        {\n            // You will need to implement your own class to properly test\n            // custom issued tokens you've setup for your end users.\n            if (!request[\"xi-api-key\"].ToString().Contains(userToken))\n            {\n                throw new AuthenticationException(\"User is not authorized\");\n            }\n        }\n    }\n\n    public static void Main(string[] args)\n    {\n        var client = new ElevenLabsClient();\n        var proxy = ElevenLabsProxyStartup.CreateDefaultHost<AuthenticationFilter>(args, client);\n        proxy.Run();\n    }\n}\nOnce you have set up your proxy server, your end users can now make authenticated requests to your proxy api instead of directly to the ElevenLabs API. The proxy server will handle authentication and forward requests to the ElevenLabs API, ensuring that your API keys and other sensitive information remain secure.\n\nText to Speech\nConvert text to speech.\n\nvar api = new ElevenLabsClient();\nvar text = \"The quick brown fox jumps over the lazy dog.\";\nvar voice = (await api.VoicesEndpoint.GetAllVoicesAsync()).FirstOrDefault();\nvar defaultVoiceSettings = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nvar clipPath = await api.TextToSpeechEndpoint.TextToSpeechAsync(text, voice, defaultVoiceSettings);\nConsole.WriteLine(clipPath);\nNote, only a single audio clip is created per text string. If you'd like to get different variations of the audio, you'll need to pass in deleteCachedFile: true.\n\nVoices\nAccess to voices created either by the user or ElevenLabs.\n\nGet All Voices\nGets a list of all available voices.\n\nvar api = new ElevenLabsClient();\nvar allVoices = await api.VoicesEndpoint.GetAllVoicesAsync();\n\nforeach (var voice in allVoices)\n{\n    Console.WriteLine($\"{voice.Id} | {voice.Name} | similarity boost: {voice.Settings?.SimilarityBoost} | stability: {voice.Settings?.Stability}\");\n}\nGet Default Voice Settings\nGets the global default voice settings.\n\nvar api = new ElevenLabsClient();\nvar result = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nConsole.WriteLine($\"stability: {result.Stability} | similarity boost: {result.SimilarityBoost}\");\nGet Voice\nvar api = new ElevenLabsClient();\nvar voice = await api.VoicesEndpoint.GetVoiceAsync(\"voiceId\");\nConsole.WriteLine($\"{voice.Id} | {voice.Name} | {voice.PreviewUrl}\");\nEdit Voice Settings\nEdit the settings for a specific voice.\n\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.EditVoiceSettingsAsync(voice, new VoiceSettings(0.7f, 0.7f));\nConsole.WriteLine($\"Was successful? {success}\");\nAdd Voice\nvar api = new ElevenLabsClient();\nvar labels = new Dictionary<string, string>\n{\n    { \"accent\", \"american\" }\n};\nvar audioSamplePaths = new List<string>();\nvar voice = await api.VoicesEndpoint.AddVoiceAsync(\"Voice Name\", audioSamplePaths, labels);\nEdit Voice\nvar api = new ElevenLabsClient();\nvar labels = new Dictionary<string, string>\n{\n    { \"age\", \"young\" }\n};\nvar audioSamplePaths = new List<string>();\nvar success = await api.VoicesEndpoint.EditVoiceAsync(voice, audioSamplePaths, labels);\nConsole.WriteLine($\"Was successful? {success}\");\nDelete Voice\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.DeleteVoiceAsync(voiceId);\nConsole.WriteLine($\"Was successful? {success}\");\nSamples\nAccess to your samples, created by you when cloning voices.\n\nGet Voice Sample\nvar api = new ElevenLabsClient();\nvar clipPath = await api.VoicesEndpoint.GetVoiceSampleAsync(voiceId, sampleId);\nConsole.WriteLine(clipPath);\nDelete Voice Sample\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.DeleteVoiceSampleAsync(voiceId, sampleId);\nConsole.WriteLine($\"Was successful? {success}\");\nHistory\nAccess to your previously synthesized audio clips including its metadata.\n\nGet History\nvar api = new ElevenLabsClient();\nvar historyItems = await api.HistoryEndpoint.GetHistoryAsync();\n\nforeach (var historyItem in historyItems.OrderBy(historyItem => historyItem.Date))\n{\n    Console.WriteLine($\"{historyItem.State} {historyItem.Date} | {historyItem.Id} | {historyItem.Text.Length} | {historyItem.Text}\");\n}\nGet History Audio\nvar api = new ElevenLabsClient();\nvar clipPath = await api.HistoryEndpoint.GetHistoryAudioAsync(historyItem);\nConsole.WriteLine(clipPath);\nDownload All History\nvar api = new ElevenLabsClient();\nvar success = await api.HistoryEndpoint.DownloadHistoryItemsAsync();\nDelete History Item\nvar api = new ElevenLabsClient();\nvar result = await api.HistoryEndpoint.DeleteHistoryItemAsync(historyItem);\nConsole.WriteLine($\"Was successful? {success}\");\nUser\nAccess to your user Information and subscription status.\n\nGet User Info\nGets information about your user account with ElevenLabs.\n\nvar api = new ElevenLabsClient();\nvar userInfo = await api.UserEndpoint.GetUserInfoAsync();\nGet Subscription Info\nGets information about your subscription with ElevenLabs.\n\nvar api = new ElevenLabsClient();\nvar subscriptionInfo = await api.UserEndpoint.GetSubscriptionInfoAsync();",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```A non-official Eleven Labs voice synthesis RESTful client for .Net https://github.com/RageAgainstThePixel/ElevenLabs-DotNet \nAbout\nA Non-Official ElevenLabs RESTful API Client for dotnet\n\nTopics\nai dotnet ml tts speech-synthesis voice-synthesis tts-api eleven-labs\nResources\n Readme\nLicense\n MIT license\nStars\n 11 stars\nWatchers\n 1 watching\nForks\n 1 fork\nReport repository\nReleases 10\nElevenLabs 1.3.0\nLatest\n3 hours ago\n+ 9 releases\nSponsor this project\n@StephenHodgson\nStephenHodgson Stephen Hodgson\nLearn more about GitHub Sponsors\nLanguages\nC#\n100.0%\n\n\n\n@StephenHodgson\nStephenHodgson ElevenLabs 1.3.0 (#14)\n\u2026\n5f7f729\n3 hours ago\nGit stats\n 15 commits\nFiles\nType\nName\nLatest commit message\nCommit time\n.github\nElevenLabs-DotNet 1.2.0 (#13)\n20 hours ago\nElevenLabs-DotNet-Proxy\nElevenLabs-DotNet 1.1.1 (#12)\n2 months ago\nElevenLabs-DotNet-Tests-Proxy\nElevenLabs-DotNet 1.2.0 (#13)\n20 hours ago\nElevenLabs-DotNet-Tests\nElevenLabs 1.3.0 (#14)\n3 hours ago\nElevenLabs-DotNet\nElevenLabs 1.3.0 (#14)\n3 hours ago\n.gitattributes\nElevenLabs-DotNet 1.0.0 (#1)\n2 months ago\n.gitignore\nElevenLabs-DotNet 1.0.0 (#1)\n2 months ago\nElevenLabs-DotNet.sln\nElevenLabs-DotNet 1.1.0 (#10)\n2 months ago\nLICENSE\nInitial commit\n2 months ago\nREADME.md\nElevenLabs-DotNet 1.2.1\n20 hours ago\nREADME.md\nElevenLabs-DotNet\nDiscord NuGet version (ElevenLabs-DotNet) NuGet version (ElevenLabs-DotNet-Proxy) Nuget Publish\n\nA non-official Eleven Labs voice synthesis RESTful client.\n\nI am not affiliated with Eleven Labs and an account with api access is required.\n\nAll copyrights, trademarks, logos, and assets are the property of their respective owners.\n\nGetting started\nInstall from NuGet\nInstall package ElevenLabs from Nuget. Here's how via command line:\n\nInstall-Package ElevenLabs-DotNet\nLooking to use ElevenLabs in the Unity Game Engine? Check out our unity package on OpenUPM:\n\nopenupm\n\nDocumentation\nTable of Contents\nAuthentication\nAPI Proxy \ud83c\udd95\nText to Speech\nVoices\nGet All Voices\nGet Default Voice Settings\nGet Voice\nEdit Voice Settings\nAdd Voice\nEdit Voice\nDelete Voice\nSamples\nGet Voice Sample\nDelete Voice Sample\nHistory\nGet History\nGet History Audio\nDownload All History\nDelete History Item\nUser\nGet User Info\nGet Subscription Info\nAuthentication\nThere are 3 ways to provide your API keys, in order of precedence:\n\nPass keys directly with constructor\nLoad key from configuration file\nUse System Environment Variables\nPass keys directly with constructor\nvar api = new ElevenLabsClient(\"yourApiKey\");\nOr create a ElevenLabsAuthentication object manually\n\nvar api = new ElevenLabsClient(new ElevenLabsAuthentication(\"yourApiKey\"));\nLoad key from configuration file\nAttempts to load api keys from a configuration file, by default .elevenlabs in the current directory, optionally traversing up the directory tree or in the user's home directory.\n\nTo create a configuration file, create a new text file named .elevenlabs and containing the line:\n\nJson format\n{\n  \"apiKey\": \"yourApiKey\",\n}\nYou can also load the file directly with known path by calling a static method in Authentication:\n\nvar api = new ElevenLabsClient(ElevenLabsAuthentication.LoadFromDirectory(\"your/path/to/.elevenlabs\"));;\nUse System Environment Variables\nUse your system's environment variables specify an api key to use.\n\nUse ELEVEN_LABS_API_KEY for your api key.\nvar api = new ElevenLabsClient(ElevenLabsAuthentication.LoadFromEnv());\nAPI Proxy\nNuGet version (ElevenLabs-DotNet-Proxy)\n\nUsing either the ElevenLabs-DotNet or com.rest.elevenlabs packages directly in your front-end app may expose your API keys and other sensitive information. To mitigate this risk, it is recommended to set up an intermediate API that makes requests to ElevenLabs on behalf of your front-end app. This library can be utilized for both front-end and intermediary host configurations, ensuring secure communication with the ElevenLabs API.\n\nFront End Example\nIn the front end example, you will need to securely authenticate your users using your preferred OAuth provider. Once the user is authenticated, exchange your custom auth token with your API key on the backend.\n\nFollow these steps:\n\nSetup a new project using either the ElevenLabs-DotNet or com.rest.elevenlabs packages.\nAuthenticate users with your OAuth provider.\nAfter successful authentication, create a new ElevenLabsAuthentication object and pass in the custom token.\nCreate a new ElevenLabsClientSettings object and specify the domain where your intermediate API is located.\nPass your new auth and settings objects to the ElevenLabsClient constructor when you create the client instance.\nHere's an example of how to set up the front end:\n\nvar authToken = await LoginAsync();\nvar auth = new ElevenLabsAuthentication(authToken);\nvar settings = new ElevenLabsClientSettings(domain: \"api.your-custom-domain.com\");\nvar api = new ElevenLabsClient(auth, settings);\nThis setup allows your front end application to securely communicate with your backend that will be using the ElevenLabs-DotNet-Proxy, which then forwards requests to the ElevenLabs API. This ensures that your ElevenLabs API keys and other sensitive information remain secure throughout the process.\n\nBack End Example\nIn this example, we demonstrate how to set up and use ElevenLabsProxyStartup in a new ASP.NET Core web app. The proxy server will handle authentication and forward requests to the ElevenLabs API, ensuring that your API keys and other sensitive information remain secure.\n\nCreate a new ASP.NET Core minimal web API project.\nAdd the ElevenLabs-DotNet nuget package to your project.\nPowershell install: Install-Package ElevenLabs-DotNet-Proxy\nManually editing .csproj: <PackageReference Include=\"ElevenLabs-DotNet-Proxy\" />\nCreate a new class that inherits from AbstractAuthenticationFilter and override the ValidateAuthentication method. This will implement the IAuthenticationFilter that you will use to check user session token against your internal server.\nIn Program.cs, create a new proxy web application by calling ElevenLabsProxyStartup.CreateDefaultHost method, passing your custom AuthenticationFilter as a type argument.\nCreate ElevenLabsAuthentication and ElevenLabsClientSettings as you would normally with your API keys, org id, or Azure settings.\npublic partial class Program\n{\n    private class AuthenticationFilter : AbstractAuthenticationFilter\n    {\n        public override void ValidateAuthentication(IHeaderDictionary request)\n        {\n            // You will need to implement your own class to properly test\n            // custom issued tokens you've setup for your end users.\n            if (!request[\"xi-api-key\"].ToString().Contains(userToken))\n            {\n                throw new AuthenticationException(\"User is not authorized\");\n            }\n        }\n    }\n\n    public static void Main(string[] args)\n    {\n        var client = new ElevenLabsClient();\n        var proxy = ElevenLabsProxyStartup.CreateDefaultHost<AuthenticationFilter>(args, client);\n        proxy.Run();\n    }\n}\nOnce you have set up your proxy server, your end users can now make authenticated requests to your proxy api instead of directly to the ElevenLabs API. The proxy server will handle authentication and forward requests to the ElevenLabs API, ensuring that your API keys and other sensitive information remain secure.\n\nText to Speech\nConvert text to speech.\n\nvar api = new ElevenLabsClient();\nvar text = \"The quick brown fox jumps over the lazy dog.\";\nvar voice = (await api.VoicesEndpoint.GetAllVoicesAsync()).FirstOrDefault();\nvar defaultVoiceSettings = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nvar clipPath = await api.TextToSpeechEndpoint.TextToSpeechAsync(text, voice, defaultVoiceSettings);\nConsole.WriteLine(clipPath);\nNote, only a single audio clip is created per text string. If you'd like to get different variations of the audio, you'll need to pass in deleteCachedFile: true.\n\nVoices\nAccess to voices created either by the user or ElevenLabs.\n\nGet All Voices\nGets a list of all available voices.\n\nvar api = new ElevenLabsClient();\nvar allVoices = await api.VoicesEndpoint.GetAllVoicesAsync();\n\nforeach (var voice in allVoices)\n{\n    Console.WriteLine($\"{voice.Id} | {voice.Name} | similarity boost: {voice.Settings?.SimilarityBoost} | stability: {voice.Settings?.Stability}\");\n}\nGet Default Voice Settings\nGets the global default voice settings.\n\nvar api = new ElevenLabsClient();\nvar result = await api.VoicesEndpoint.GetDefaultVoiceSettingsAsync();\nConsole.WriteLine($\"stability: {result.Stability} | similarity boost: {result.SimilarityBoost}\");\nGet Voice\nvar api = new ElevenLabsClient();\nvar voice = await api.VoicesEndpoint.GetVoiceAsync(\"voiceId\");\nConsole.WriteLine($\"{voice.Id} | {voice.Name} | {voice.PreviewUrl}\");\nEdit Voice Settings\nEdit the settings for a specific voice.\n\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.EditVoiceSettingsAsync(voice, new VoiceSettings(0.7f, 0.7f));\nConsole.WriteLine($\"Was successful? {success}\");\nAdd Voice\nvar api = new ElevenLabsClient();\nvar labels = new Dictionary<string, string>\n{\n    { \"accent\", \"american\" }\n};\nvar audioSamplePaths = new List<string>();\nvar voice = await api.VoicesEndpoint.AddVoiceAsync(\"Voice Name\", audioSamplePaths, labels);\nEdit Voice\nvar api = new ElevenLabsClient();\nvar labels = new Dictionary<string, string>\n{\n    { \"age\", \"young\" }\n};\nvar audioSamplePaths = new List<string>();\nvar success = await api.VoicesEndpoint.EditVoiceAsync(voice, audioSamplePaths, labels);\nConsole.WriteLine($\"Was successful? {success}\");\nDelete Voice\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.DeleteVoiceAsync(voiceId);\nConsole.WriteLine($\"Was successful? {success}\");\nSamples\nAccess to your samples, created by you when cloning voices.\n\nGet Voice Sample\nvar api = new ElevenLabsClient();\nvar clipPath = await api.VoicesEndpoint.GetVoiceSampleAsync(voiceId, sampleId);\nConsole.WriteLine(clipPath);\nDelete Voice Sample\nvar api = new ElevenLabsClient();\nvar success = await api.VoicesEndpoint.DeleteVoiceSampleAsync(voiceId, sampleId);\nConsole.WriteLine($\"Was successful? {success}\");\nHistory\nAccess to your previously synthesized audio clips including its metadata.\n\nGet History\nvar api = new ElevenLabsClient();\nvar historyItems = await api.HistoryEndpoint.GetHistoryAsync();\n\nforeach (var historyItem in historyItems.OrderBy(historyItem => historyItem.Date))\n{\n    Console.WriteLine($\"{historyItem.State} {historyItem.Date} | {historyItem.Id} | {historyItem.Text.Length} | {historyItem.Text}\");\n}\nGet History Audio\nvar api = new ElevenLabsClient();\nvar clipPath = await api.HistoryEndpoint.GetHistoryAudioAsync(historyItem);\nConsole.WriteLine(clipPath);\nDownload All History\nvar api = new ElevenLabsClient();\nvar success = await api.HistoryEndpoint.DownloadHistoryItemsAsync();\nDelete History Item\nvar api = new ElevenLabsClient();\nvar result = await api.HistoryEndpoint.DeleteHistoryItemAsync(historyItem);\nConsole.WriteLine($\"Was successful? {success}\");\nUser\nAccess to your user Information and subscription status.\n\nGet User Info\nGets information about your user account with ElevenLabs.\n\nvar api = new ElevenLabsClient();\nvar userInfo = await api.UserEndpoint.GetUserInfoAsync();\nGet Subscription Info\nGets information about your subscription with ElevenLabs.\n\nvar api = new ElevenLabsClient();\nvar subscriptionInfo = await api.UserEndpoint.GetSubscriptionInfoAsync();\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs' API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information.\n2) The text is a description of a non-official ElevenLabs RESTful API client for .Net, available on GitHub. It includes information on how to install and use the client, as well as details on authentication and accessing various endpoints for text-to-speech, voices, samples, history, and user information.\n3) \n- The text describes a non-official ElevenLabs RESTful API client for .Net available on GitHub.\n- The client provides access to various endpoints for text-to-speech, voices, samples, history, and user information.\n- The text provides information on how to install and use the client, including details on authentication.\n- The client can be authenticated using API keys passed directly with the constructor, loaded from a configuration file, or using system environment variables.\n- The text provides examples of how to use the client to access various endpoints, including getting all available voices, getting default voice settings, editing voice settings, adding and editing voices, and accessing samples and history.\n- The text also includes information on using an API proxy to mitigate the risk of exposing API keys and other sensitive information."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "The text describes the use of several endpoints, including:\n\n- POST /v1/text-to-speech/{voice_id}: for converting text to speech\n- GET /v1/voices: for accessing all available voices\n- GET /v1/voices/settings/default: for getting the global default voice settings\n- GET /v1/voices/{voice_id}: for getting a specific voice\n- POST /v1/voices/{voice_id}/settings/edit: for editing the settings for a specific voice\n- POST /v1/voices/add: for adding a new voice\n- POST /v1/voices/{voice_id}/edit: for editing an existing voice\n- DELETE /v1/voices/{voice_id}: for deleting a voice\n- GET /v1/voices/{voice_id}/samples/{sample_id}/audio: for getting audio from a voice sample\n- DELETE /v1/voices/{voice_id}/samples/{sample_id}: for deleting a voice sample\n- GET /v1/history: for accessing previously synthesized audio clips and their metadata\n- GET /v1/history/{history_item_id}/audio: for getting audio from a specific history item\n- DELETE /v1/history/{history_item_id}: for deleting a history item\n- GET /v1/user: for accessing user information\n- GET /v1/user/subscription: for accessing subscription information."
            }
        ]
    },
    {
        "title": "Flutter app using ElevenLabs TTS: https://github.com/LunarioTK/ElevenLabs_tts",
        "text": "About\nUsing Eleven Labs api to read text.\n\nResources\n Readme\nStars\n 1 star\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nC++\n38.2%\n \nCMake\n30.9%\n \nDart\n22.9%\n \nHTML\n3.1%\n \nC\n2.3%\n \nSwift\n2.3%\n \nOther\n0.3%\n\n\n\n\nKelvin Rodrigues Update\nf95a96f\non Feb 9\nGit stats\n 7 commits\nFiles\nType\nName\nLatest commit message\nCommit time\nandroid\nEleven labs tts app\n3 months ago\nios\nEleven labs tts app\n3 months ago\nlib\nUpdate\n3 months ago\nlinux\nEleven Labs api working\n3 months ago\nmacos\nEleven Labs api working\n3 months ago\ntest\nEleven labs tts app\n3 months ago\nweb\nEleven labs tts app\n3 months ago\nwindows\nEleven Labs api working\n3 months ago\n.gitignore\nEleven labs tts app\n3 months ago\n.metadata\nEleven labs tts app\n3 months ago\nREADME.md\nEleven labs tts app\n3 months ago\nanalysis_options.yaml\nEleven labs tts app\n3 months ago\npubspec.lock\nPanel Added\n3 months ago\npubspec.yaml\nPanel Added\n3 months ago\nREADME.md\nelevenlabs_tts_app\nA new Flutter project.\n\nGetting Started\nThis project is a starting point for a Flutter application.\n\nA few resources to get you started if this is your first Flutter project:\n\nLab: Write your first Flutter app\nCookbook: Useful Flutter samples\nFor help getting started with Flutter development, view the online documentation, which offers tutorials, samples, guidance on mobile development, and a full API reference.",
        "messages": [
            {
                "role": "user",
                "content": "Hey, can you help me analyze this text? Take on the persona of AnalysisBot."
            },
            {
                "role": "assistant",
                "content": "Sure! I am now AnalysisBot. Anything else you want me to know before I get to work?."
            },
            {
                "role": "user",
                "content": "Yes. Here is some background info:\n```background.txt\nIt's about ElevenLabs and their API. Here's the documentation:\n\nPOST\n/v1/text-to-speech/{voice_id}\nText To Speech\n\nPOST\n/v1/text-to-speech/{voice_id}/stream\nText To Speech\nmodels\nAccess the different models of the platform.\n\n\n\nGET\n/v1/models\nGet Models\nvoices\nAccess to voices created either by you or us.\n\n\n\nGET\n/v1/voices\nGet Voices\n\nGET\n/v1/voices/settings/default\nGet Default Voice Settings.\n\nGET\n/v1/voices/{voice_id}/settings\nGet Voice Settings\n\nGET\n/v1/voices/{voice_id}\nGet Voice\n\nDELETE\n/v1/voices/{voice_id}\nDelete Voice\n\nPOST\n/v1/voices/{voice_id}/settings/edit\nEdit Voice Settings\n\nPOST\n/v1/voices/add\nAdd Voice\n\nPOST\n/v1/voices/{voice_id}/edit\nEdit Voice\nsamples\nAccess to your samples. A sample is any audio file you attached to a voice. A voice can have one or more samples.\n\n\n\nDELETE\n/v1/voices/{voice_id}/samples/{sample_id}\nDelete Sample\n\nGET\n/v1/voices/{voice_id}/samples/{sample_id}/audio\nGet Audio From Sample\nhistory\nAccesses your history. Your history is a list of all your created audio including its metadata.\n\n\n\nGET\n/v1/history\nGet Generated Items\n\nGET\n/v1/history/{history_item_id}/audio\nGet Audio From History Item\n\nDELETE\n/v1/history/{history_item_id}\nDelete History Item\n\nPOST\n/v1/history/download\nDownload History Items\nuser\n\n\nGET\n/v1/user/subscription\nGet User Subscription Info\n\nGET\n/v1/user\nGet User Info\n```\n\nAnd here is the text I want you to analyze:\n```Flutter app using ElevenLabs TTS: https://github.com/LunarioTK/ElevenLabs_tts\nAbout\nUsing Eleven Labs api to read text.\n\nResources\n Readme\nStars\n 1 star\nWatchers\n 1 watching\nForks\n 0 forks\nReport repository\nReleases\nNo releases published\nPackages\nNo packages published\nLanguages\nC++\n38.2%\n \nCMake\n30.9%\n \nDart\n22.9%\n \nHTML\n3.1%\n \nC\n2.3%\n \nSwift\n2.3%\n \nOther\n0.3%\n\n\n\n\nKelvin Rodrigues Update\nf95a96f\non Feb 9\nGit stats\n 7 commits\nFiles\nType\nName\nLatest commit message\nCommit time\nandroid\nEleven labs tts app\n3 months ago\nios\nEleven labs tts app\n3 months ago\nlib\nUpdate\n3 months ago\nlinux\nEleven Labs api working\n3 months ago\nmacos\nEleven Labs api working\n3 months ago\ntest\nEleven labs tts app\n3 months ago\nweb\nEleven labs tts app\n3 months ago\nwindows\nEleven Labs api working\n3 months ago\n.gitignore\nEleven labs tts app\n3 months ago\n.metadata\nEleven labs tts app\n3 months ago\nREADME.md\nEleven labs tts app\n3 months ago\nanalysis_options.yaml\nEleven labs tts app\n3 months ago\npubspec.lock\nPanel Added\n3 months ago\npubspec.yaml\nPanel Added\n3 months ago\nREADME.md\nelevenlabs_tts_app\nA new Flutter project.\n\nGetting Started\nThis project is a starting point for a Flutter application.\n\nA few resources to get you started if this is your first Flutter project:\n\nLab: Write your first Flutter app\nCookbook: Useful Flutter samples\nFor help getting started with Flutter development, view the online documentation, which offers tutorials, samples, guidance on mobile development, and a full API reference.\n```\n\nExplain it in relation to the background info. Do it in 3 steps:\n1) \"General description of the background info\"\n2) \"General description of the text\"\n3) \"Bulletpoint breakdown of the text\""
            },
            {
                "role": "assistant",
                "content": "1) The background info provides documentation for ElevenLabs API, which includes endpoints for text-to-speech, accessing models and voices, managing samples and history, and user information.\n\n2) The text is a GitHub repository for a Flutter app that uses ElevenLabs TTS API to read text. It includes information on the project's resources, Git stats, and file structure.\n\n3) \n- The GitHub repository is for a Flutter app that uses ElevenLabs TTS API to read text.\n- The app uses the API's text-to-speech endpoint.\n- The repository includes information on the project's resources, such as the README and Git stats.\n- The app is written in C++, CMake, Dart, HTML, C, Swift, and other languages.\n- The repository includes files for different platforms, such as Android, iOS, Linux, macOS, and Windows.\n- The app's file structure includes directories for lib, test, and web.\n- The README includes information on getting started with Flutter development and links to resources such as the Flutter Lab and Cookbook."
            },
            {
                "role": "user",
                "content": "Which endpoints are used?"
            },
            {
                "role": "assistant",
                "content": "Based on the information provided in the text, it appears that the app uses the text-to-speech endpoint of ElevenLabs TTS API. However, it is not clear if any other endpoints are used."
            }
        ]
    }
]